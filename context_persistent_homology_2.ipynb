{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological Collocation and Keyword-Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy gudhi -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context vector is a critical part of the multihead attention mechanism in transformers. As indicated by the provided formula:\n",
    "\n",
    "$$\\mathbf{c}_{i}^{l,h} = \\sum_{j=1}^{n} a_{i, j}^{l,h} \\mathbf{v}_{j}^{l,h}$$\n",
    "\n",
    "the context vector $\\mathbf{c}_{i, l}^h$ for the $h^{th}$ head in layer $l$ is computed as a weighted sum of the value vectors $\\mathbf{v}_{j}^{l,h}$, where the weights are the attention scores $a_{ij}^{l,h}$. Here, $n$ is the number of value vectors, corresponding to the number of input tokens for the attention mechanism.\n",
    "\n",
    "The attention scores $a_{ij}^{l,h}$ for attention head $k$ in layer $l$, themselves are computed using the query and key vectors. In the scaled dot-product attention mechanism typically used in transformers, the attention score between the $i^{th}$ query $\\mathbf{q}_i^{l,h}$ and the $j^{th}$ key $\\mathbf{k}_j^{l,h}$ is computed as:\n",
    "\n",
    "$$a_{ij}^{l,h} = \\frac{\\exp(\\mathbf{q}_i^{l,h} \\cdot \\mathbf{k}_j^{l,h} / \\sqrt{d})}{\\sum_{m=1}^{n} \\exp(\\mathbf{q}_i^{l,h} \\cdot \\mathbf{k}_m^{l,h} / \\sqrt{d})}$$\n",
    "\n",
    "where $d$ is the dimensionality of the queries and keys, and $\\cdot$ denotes the dot product. The division by $\\sqrt{d}$ is a scaling factor that is used to prevent the dot product from growing too large in magnitude, which could lead to vanishing gradients during training. The softmax function is applied to the raw attention scores to ensure that they sum up to $1$, allowing them to be interpreted as probabilities or relative importances.\n",
    "\n",
    "The queries, keys, and values are themselves computed by applying learned linear transformations to the input embeddings. If $\\mathbf{x}_i$ denotes the input embedding for the $i^{th}$ token, then we have:\n",
    "\n",
    "$$\\mathbf{q}_i^{l,h} = W_Q^{l, h} \\mathbf{x}_i$$\n",
    "$$\\mathbf{k}_i^{l,h} = W_K^{l, h} \\mathbf{x}_i$$\n",
    "$$\\mathbf{v}_i^{l,h} = W_V^{l, h} \\mathbf{x}_i$$\n",
    "\n",
    "where $W_Q^{l,h}$, $W_K^{l,h}$, and $W_V^{l,h}$ are the weight matrices for the queries, keys, and values, respectively, for the $k^{th}$ head in the $l^{th}$ layer.\n",
    "\n",
    "The context vectors $\\mathbf{c}_{i, l}^h$ provide a summary of the input tokens, weighted by their relevance to the query. They can be thought of as a form of \"contextualized\" embedding, where the context is determined by the other tokens in the input sequence and their interaction weight given by the attention matrix $a_{ij}^{l,h}$ for head $h \\in \\{1, 2, ..., \\mathcal{H}\\}$. The multihead attention mechanism allows the model to capture different types of relevance or \"attention\" by using multiple heads, each with its own learned linear transformations.\n",
    "\n",
    "In the multihead attention mechanism, the attention operation is not performed just once, but multiple times in parallel. The queries, keys, and values are transformed with different learned linear projections to $h$ different sets of queries, keys, and values, where $h$ is the number of heads. Then the attention mechanism is applied to each of these sets, yielding $h$ output vectors, which are then concatenated and linearly transformed to result in the final output.\n",
    "\n",
    "Let $W_Q^{l,h}$, $W_K^{l,h}$, and $W_V^{l,h}$ denote the weight matrices for the $h^{th}$ head for the queries, keys, and values, respectively, in the $l^{th}$ layer, and let $W_O^{l}$ denote the output weight matrix for layer $l$. Then the output of the multihead attention mechanism for layer $l$ is computed as:\n",
    "\n",
    "$$\\mathbf{c}_i^l = W_O^{l} [\\mathbf{c}_{i}^{l,h}; \\mathbf{c}_{i}^{l,h}; \\ldots; \\mathbf{c}_{i}^{l, \\mathcal{H}}]$$\n",
    "\n",
    "where $\\mathbf{c}_{i}^{l,h}$ is the output of the attention mechanism for the $h^{th}$ head in layer $l$, for token $x_i$, computed as:\n",
    "\n",
    "$$\\mathbf{c}_{i}^{l,h} = \\sum_{j=1}^{n} a_{ij}^{l,h} \\mathbf{v}_{j}^{l,h}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def compute_attention_matrix_gpt_2(sentence, layer, head):\n",
    "    # Load pretrained model/tokenizer\n",
    "    model = GPT2Model.from_pretrained('gpt2', output_attentions=True)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Tokenize input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "    # Pass through model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Extract the attention weights for the specified layer and head\n",
    "    attention_matrix = attentions[layer][0, head].detach().numpy()\n",
    "\n",
    "    return attention_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.9799156  0.02008435 0.         0.        ]\n",
      " [0.81174487 0.13985004 0.04840514 0.        ]\n",
      " [0.33955273 0.15688916 0.21757792 0.2859802 ]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Deep learning is fascinating\"\n",
    "layer = 2\n",
    "head = 3\n",
    "\n",
    "gpt_2_attention_matrix = compute_attention_matrix_gpt_2(sentence, layer, head)\n",
    "print(gpt_2_attention_matrix.shape)\n",
    "print(gpt_2_attention_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def compute_attention_matrix_bert(sentence, layer, head):\n",
    "    # Load pretrained model/tokenizer\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "    # Pass through model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Extract the attention weights for the specified layer and head\n",
    "    attention_matrix = attentions[layer][0, head].detach().numpy()\n",
    "\n",
    "    return attention_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6)\n",
      "[[0.65833837 0.02821147 0.05961464 0.06537832 0.06500569 0.12345145]\n",
      " [0.7571759  0.00225254 0.01708878 0.0295937  0.02422678 0.1696624 ]\n",
      " [0.5111288  0.01439373 0.00574702 0.07166345 0.06277563 0.33429137]\n",
      " [0.42424822 0.03885158 0.04304642 0.08848479 0.0646913  0.3406777 ]\n",
      " [0.71238786 0.01339824 0.05244174 0.03502652 0.01661684 0.17012876]\n",
      " [0.7925911  0.01647069 0.035555   0.02690707 0.03360174 0.09487453]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Deep learning is fascinating\"\n",
    "layer = 2\n",
    "head = 3\n",
    "\n",
    "bert_attention_matrix = compute_attention_matrix_bert(sentence, layer, head)\n",
    "print(bert_attention_matrix.shape)\n",
    "print(bert_attention_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each transformer layer comprises a multihead self-attention mechanism, followed by layer normalization, a position-wise feed-forward network, and another layer normalization. Let's denote the $l^{th}$ transformer layer in the model, where $l$ ranges from 1 to $L$ (with $L$ being the total number of layers). \n",
    "\n",
    "1. **Multihead Self-Attention Mechanism**: The multihead self-attention mechanism in the $l^{th}$ layer operates on the input embeddings $\\mathbf{x}_i^l$, transforming them into queries $\\mathbf{q}_i^{l,h}$, keys $\\mathbf{k}_i^{l,h}$, and values $\\mathbf{v}_i^{l,h}$ for each head $h$ as before. Then the context vectors are computed, and concatenated as before, and the weight matrix $W_O^l$ is applied to get $\\mathbf{c}_i^l$\n",
    "\n",
    "2. **Layer Normalization**: Layer normalization stabilizes the learning process and reduces internal covariate shift by normalizing the multihead self-attention output across the hidden dimension. For the $l^{th}$ layer, this is computed as:\n",
    "\n",
    "    $$\\mathbf{c'}_i^l = \\frac{\\mathbf{c}_i^l - \\mu^{l}}{\\sigma^{l}}$$\n",
    "\n",
    "   where $\\mu^{l}$ and $\\sigma^{l}$ are the mean and standard deviation of the layer outputs, computed as:\n",
    "\n",
    "    $$\\mu^{l} = \\frac{1}{H}\\sum^{H}_{i=1}c_{i}^l$$\n",
    "    $$\\sigma^{l} = \\sqrt{\\frac{1}{H}\\sum^{H}_{i=1}\\left(c_{i}^l-\\mu^{l}\\right)^{2}}$$\n",
    "\n",
    "3. **Position-Wise Feed-Forward Network (FFN)**: The output of the layer normalization is then passed through a position-wise feed-forward network (FFN). The\n",
    "\n",
    "The output of the layer normalization is then passed through a position-wise feed-forward network (FFN). The FFN consists of two linear transformations with a ReLU activation in between. For each position $i$, the FFN is applied to $\\mathbf{c}_i'^l$ independently. Let's denote the weight matrices and bias vectors of the two linear transformations as $W_1^l$, $b_1^l$, $W_2^l$, and $b_2^l$, respectively. Then the output of the FFN is computed as:\n",
    "\n",
    "$$\\mathbf{d}_i^l = W_2^l \\mathbf{max}(0, W_1^l \\mathbf{c'}_i^l + b_1^l) + b_2^l$$\n",
    "\n",
    "where $\\max(0, x)$ denotes the ReLU activation function.\n",
    "\n",
    "4. **Second Layer Normalization**: Finally, the output of the FFN goes through another layer normalization step to compute the final output of the transformer layer:\n",
    "\n",
    "$$\\mathbf{o}_i^l = \\frac{\\mathbf{d}_i^l - \\mu^l}{\\sigma^l}$$\n",
    "\n",
    "where again, $\\mu^l$ and $\\sigma^l$ are the mean and standard deviation of the layer outputs, computed similarly as before but now with $\\mathbf{d}_i^l$ instead of $\\mathbf{c}_i^l$.\n",
    "\n",
    "In the following code, the `outputs.hidden_states[layer]` attribute in the context of transformer models typically refers to the matrix $O_l = [\\mathbf{o}_1^l; \\mathbf{o}_2^l; \\ldots; \\mathbf{o}_n^l]$, where $n$ is the number of input tokens, and $L$ is the total number of layers in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n",
      "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.0594,  0.6532, -0.1400,  ..., -0.8445,  0.6568, -0.2210],\n",
      "         [-1.5690, -0.1996,  0.0325,  ...,  0.4756,  0.2562, -0.7868],\n",
      "         [-0.4970, -0.2288, -0.4677,  ...,  0.4528,  0.2378,  0.5511],\n",
      "         [ 0.8615,  0.3349,  0.3782,  ...,  0.0857, -0.1091,  0.5007],\n",
      "         [-0.3251, -0.3188, -0.1163,  ..., -0.3960,  0.4112, -0.0776]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Deep learning is fascinating\", return_tensors=\"pt\")\n",
    "\n",
    "# Specify `output_hidden_states=True` when calling the model\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# Now you can access the hidden states at a specific layer\n",
    "layer = 0\n",
    "hidden_states = outputs.hidden_states[layer]\n",
    "print(hidden_states.shape)\n",
    "print(hidden_states)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reintroduce the BERT model in slightly different notation now, using matrices to collect all of the queries, keys, and values. \n",
    "\n",
    "**1. Word Embeddings and Positional Encoding**\n",
    "\n",
    "Given a sequence of input tokens, $X = \\{x_1, x_2, ..., x_n\\}$, each token is converted into a word embedding vector using an embedding lookup table $E$. Hence, the word embeddings $Z$ are calculated as:\n",
    "\n",
    "$$ Z = \\{E(x_1), E(x_2), ..., E(x_n)\\} $$\n",
    "\n",
    "In BERT, the word embeddings are then summed with a positional encoding $P$ to retain the positional information, giving us:\n",
    "\n",
    "$$ Z' = Z + P $$\n",
    "\n",
    "**2. Transformer Encoder Layers**\n",
    "\n",
    "A Transformer encoder consists of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network. For the input to the $l^{th}$ layer, denoted $H^{(l)}$, we have $H^{(1)} = Z'$ for the first layer.\n",
    "\n",
    "**2.1. Multi-Head Self-Attention**\n",
    "\n",
    "The multi-head self-attention mechanism computes a weighted sum of the input vectors, where the weights are determined by the attention scores between the token and all tokens in the sequence.\n",
    "\n",
    "In the self-attention mechanism, three sets of learned linear transformations are applied to the input vectors $H^{(l)}$ to compute the query $Q^{(l, h)}$, key $K^{(l, h)}$, and value $V^{(l, h)}$ vectors for each head $h$ in the layer $l$:\n",
    "\n",
    "$$Q^{(l, h)} = H^{(l)}W_{Q}^{l, h}$$\n",
    "$$K^{(l, h)} = H^{(l)}W_{K}^{l, h}$$\n",
    "$$V^{(l, h)} = H^{(l)}W_{V}^{l, h}$$\n",
    "\n",
    "where $W_{q}^{(l, h)}$, $W_{k}^{(l, h)}$, and $W_{v}^{(l, h)}$ are the weight matrices for the query, key, and value transformations at the $l^{th}$ layer for the $h^{th}$ head.\n",
    "\n",
    "Then, the attention scores $S^{(l, h)}$ are computed by taking the dot product of the query and key vectors, followed by a softmax:\n",
    "\n",
    "$$ S^{(l, h)} = \\text{softmax}\\left(\\frac{Q^{(l, h)}{K^{(l, h)}}^T}{\\sqrt{d_k}}\\right) $$\n",
    "\n",
    "where $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "The output of the self-attention mechanism for each head is a weighted sum of the value vectors, using the attention scores as weights:\n",
    "\n",
    "$$ O^{(l, h)} = S^{(l, h)}V^{(l, h)} $$\n",
    "\n",
    "The output vectors of all heads are then concatenated and linearly transformed to result in the final output of the multi-head self-attention mechanism:\n",
    "\n",
    "$$ A^{(l)} = [O^{(l, 1)}, ..., O^{(l, \\mathcal{H})}]W_{O}^{(l)} $$\n",
    "\n",
    "where $\\mathcal{H}$ is the number of heads, and $W_{o}^{(l)}$ is a learned weight matrix.\n",
    "\n",
    "**2.2. Position-Wise Feed-Forward Networks**\n",
    "\n",
    "The output of the multi-head self-attention mechanism is then passed through a position-wise feed-forward network (FFN).\n",
    "\n",
    "Apologies for the abrupt cut-off. Let's continue with the explanation.\n",
    "\n",
    "This network is applied independently to each position:\n",
    "\n",
    "$$ F^{(l)} = \\text{FFN}(A^{(l)}) $$\n",
    "\n",
    "where FFN represents the feed-forward network.\n",
    "\n",
    "**2.3. Residual Connections and Layer Normalization**\n",
    "\n",
    "In the case of a Transformer encoder layer, there are two sub-layers:\n",
    "\n",
    "1. The multi-head self-attention mechanism\n",
    "2. The position-wise feed-forward network\n",
    "\n",
    "Each of these sub-layers uses a shortcut connection (residual connection), and the output of each is normalized separately. So, what the notation is trying to express is that the output of the $l^{th}$ layer before and after each sub-layer is normalized separately.\n",
    "\n",
    "Let's denote the output of the self-attention mechanism as $A^{(l)}$ and the output of the feed-forward network as $F^{(l)}$. Now, let's denote the input to the $l^{th}$ layer as $H^{(l)}$, and the output of the $l^{th}$ layer as $H^{(l+1)}$. \n",
    "\n",
    "The processing of the $l^{th}$ layer can then be expressed as follows:\n",
    "\n",
    "1. Multi-Head Self-Attention:\n",
    "\n",
    "    $$ A^{(l)} = \\text{MultiHeadSelfAttention}(H^{(l)}) $$\n",
    "    $$ H_{\\text{intermediate}}^{(l+1)} = \\text{LayerNorm}(H^{(l)} + A^{(l)}) $$\n",
    "\n",
    "    Here, $H_{\\text{intermediate}}^{(l+1)}$ is the output after the self-attention mechanism, including the residual connection and layer normalization.\n",
    "\n",
    "2. Position-Wise Feed-Forward Networks:\n",
    "\n",
    "    $$ F^{(l)} = \\text{FFN}(H_{\\text{intermediate}}^{(l+1)}) $$\n",
    "    $$ H^{(l+1)} = \\text{LayerNorm}(H_{\\text{intermediate}}^{(l+1)} + F^{(l)}) $$\n",
    "\n",
    "    Here, the FFN is applied to $H_{\\text{intermediate}}^{(l+1)}$, not $H^{(l)}$.\n",
    "\n",
    "So, $H^{(l+1)}$ is the result of the entire $l^{th}$ layer, which includes both the self-attention and feed-forward sub-layers. The residual connections and layer normalization operations are applied separately after each sub-layer. There are two steps in the process, and layer normalization is applied after each step.\n",
    "\n",
    "**3. Output**\n",
    "\n",
    "The output of the final layer $L$ of the Transformer encoder is the sequence of vectors $H^{(L+1)}$, which serves as the contextualized representation of the input tokens. This corresponds to `outputs.last_hidden_state` in the code.\n",
    "\n",
    "If we denote `outputs.last_hidden_state` as $H$, then $H[0]$ corresponds to the contextualized representation of the first token in the input sequence.\n",
    "\n",
    "Please note that the above explanation is a simplification of the actual computations involved in the BERT model. BERT also incorporates several other features, such as token type embeddings for distinguishing different sentences in the same input, and a special [CLS] token at the beginning of the input for classification tasks. \n",
    "\n",
    "The next block of code computes all of the context vectors for all of the `layer`s and `head`s at once, but we just want to look at, \n",
    "\n",
    "$$ O^{(l, h)} = S^{(l, h)}V^{(l, h)}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def compute_output_1(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n",
      "tensor([[[ 0.6161,  0.0203,  0.0927,  ..., -0.0607,  0.2400, -0.0631],\n",
      "         [ 0.5375,  0.0317,  0.0341,  ..., -0.1485,  0.2741, -0.1216],\n",
      "         [ 0.4041,  0.0027,  0.0068,  ..., -0.2113,  0.2521, -0.1607],\n",
      "         [ 0.5031,  0.0235, -0.0346,  ..., -0.2013,  0.2198, -0.1369],\n",
      "         [ 0.2977,  0.0328,  0.0355,  ..., -0.2225,  0.2670, -0.1371],\n",
      "         [ 0.6082,  0.0109,  0.0203,  ..., -0.1514,  0.2542, -0.1082]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "sentence = \"Deep learning is fascinating\"\n",
    "layer = 0  # Layer index\n",
    "head = 0  # Head index\n",
    "output = compute_output_1(sentence, layer, head)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Transformer model, each head has its own weight matrices $W_Q^{l,h}$, $W_K^{l,h}$, and $W_V^{l,h}$ for query, key, and value transformations, respectively. However, in the implementation of Hugging Face's transformers, these weights are stored in one combined tensor for efficiency. Specifically, these weight matrices are combined along the output dimension (i.e., the dimension corresponding to the hidden size), so the shape of the combined tensor is [hidden_size, hidden_size].\n",
    "\n",
    "To obtain the weight matrix for a specific head, we need to split this combined tensor along the output dimension and select the corresponding portion. Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n",
      "tensor([[[ 0.6161,  0.0203,  0.0927,  0.1709, -0.5999,  0.2016,  0.2058,\n",
      "           0.0541,  0.0303,  0.0732, -0.4342,  0.0593, -0.1368,  0.1146,\n",
      "           0.0990,  0.1922, -0.2231,  0.2022, -0.4174,  0.0530,  0.2061,\n",
      "           0.0966,  0.0493,  0.2927,  0.1682,  0.0812, -0.3396,  0.0712,\n",
      "           0.3388,  0.0566, -0.0778, -0.0397, -0.4107,  0.2250,  0.1740,\n",
      "          -0.0221,  0.2643, -0.2039,  0.1421,  0.3994, -0.1633, -0.2542,\n",
      "           0.2551,  0.3152, -0.0846,  0.0445,  0.3512, -0.4743, -0.2733,\n",
      "           0.3233, -0.1005,  0.1634, -0.4508, -0.1815,  0.1717, -0.0284,\n",
      "           0.4119,  0.2804,  0.0192, -0.4636,  0.1365,  0.4404,  0.0805,\n",
      "           0.3921],\n",
      "         [ 0.5375,  0.0317,  0.0341,  0.2105, -0.4331,  0.1121,  0.1402,\n",
      "           0.0348,  0.0429,  0.1176, -0.3509,  0.2004, -0.1399, -0.0059,\n",
      "          -0.0320,  0.1172, -0.1960,  0.3601, -0.2545, -0.0250,  0.3612,\n",
      "           0.0695, -0.1173,  0.2478,  0.0480,  0.0372, -0.2670, -0.1288,\n",
      "           0.3411, -0.0203, -0.0871,  0.0276, -0.3599,  0.1820,  0.1591,\n",
      "           0.0439,  0.1536, -0.3497,  0.3067,  0.1662, -0.1895, -0.0588,\n",
      "           0.4188,  0.4520, -0.0886,  0.1338,  0.5615, -0.5109, -0.2798,\n",
      "           0.2708,  0.0250,  0.0734, -0.5004, -0.2492,  0.1747,  0.0451,\n",
      "           0.3286,  0.3377, -0.1529, -0.4526,  0.1118,  0.2362,  0.1408,\n",
      "           0.5269],\n",
      "         [ 0.4041,  0.0027,  0.0068,  0.1828, -0.2962,  0.0500,  0.1882,\n",
      "           0.0747,  0.1247,  0.0869, -0.3403,  0.2452, -0.2185, -0.1453,\n",
      "          -0.0423,  0.1741, -0.1504,  0.3656, -0.2917, -0.0026,  0.2365,\n",
      "           0.0880, -0.2588,  0.2036,  0.0270,  0.0543, -0.2084, -0.1616,\n",
      "           0.4270, -0.1352, -0.0539,  0.0844, -0.3185,  0.1754,  0.1290,\n",
      "           0.1302,  0.2210, -0.4983,  0.4212,  0.1351, -0.1734, -0.0916,\n",
      "           0.4088,  0.4272, -0.1265,  0.2066,  0.7980, -0.5218, -0.1618,\n",
      "           0.1236,  0.0701,  0.1429, -0.4903, -0.2933,  0.0924,  0.0041,\n",
      "           0.3081,  0.4839, -0.1535, -0.5497,  0.0769,  0.1768,  0.1409,\n",
      "           0.4908],\n",
      "         [ 0.5031,  0.0235, -0.0346,  0.1283, -0.2587,  0.0527,  0.1295,\n",
      "           0.0793,  0.0927,  0.0812, -0.3513,  0.2519, -0.1839, -0.1738,\n",
      "          -0.0486,  0.0847, -0.0768,  0.3851, -0.1947, -0.1093,  0.3915,\n",
      "           0.0847, -0.2999,  0.2412, -0.0574,  0.0099, -0.2435, -0.2035,\n",
      "           0.4028, -0.2090, -0.0512,  0.0687, -0.2702,  0.1810,  0.1778,\n",
      "           0.1964,  0.1775, -0.5433,  0.5620,  0.0250, -0.2828, -0.0387,\n",
      "           0.4722,  0.4977, -0.1323,  0.2725,  0.7391, -0.5122, -0.2249,\n",
      "           0.1478,  0.1286,  0.0424, -0.5614, -0.3252,  0.0453,  0.0481,\n",
      "           0.2881,  0.4258, -0.2677, -0.5560, -0.0265,  0.0924,  0.1306,\n",
      "           0.4806],\n",
      "         [ 0.2977,  0.0328,  0.0355,  0.2448, -0.3030,  0.1036,  0.2200,\n",
      "           0.0619,  0.1313,  0.0533, -0.3506,  0.1833, -0.2324, -0.0782,\n",
      "          -0.0073,  0.1611, -0.2175,  0.3482, -0.3455,  0.0269,  0.1893,\n",
      "           0.0844, -0.1152,  0.1929,  0.0208,  0.0865, -0.2423, -0.1579,\n",
      "           0.4345, -0.1037, -0.0547,  0.0971, -0.2931,  0.1593,  0.0891,\n",
      "           0.0645,  0.2483, -0.4757,  0.3613,  0.1782, -0.1767, -0.1078,\n",
      "           0.3686,  0.3976, -0.1136,  0.1569,  0.7946, -0.5379, -0.1262,\n",
      "           0.1743,  0.0026,  0.1811, -0.4313, -0.2597,  0.1868, -0.0555,\n",
      "           0.2718,  0.4528, -0.0163, -0.5456,  0.1180,  0.2644,  0.1737,\n",
      "           0.4698],\n",
      "         [ 0.6082,  0.0109,  0.0203,  0.1795, -0.4470,  0.1281,  0.1042,\n",
      "           0.0514,  0.0542,  0.0803, -0.3888,  0.2003, -0.1889, -0.0418,\n",
      "          -0.0069,  0.1271, -0.1739,  0.3278, -0.3282, -0.0202,  0.2728,\n",
      "           0.0836, -0.0858,  0.2612,  0.0444,  0.0650, -0.2623, -0.0545,\n",
      "           0.3986, -0.0532, -0.1043,  0.0298, -0.3396,  0.1822,  0.1583,\n",
      "           0.0585,  0.1788, -0.3691,  0.3124,  0.2055, -0.2368, -0.1318,\n",
      "           0.3675,  0.3906, -0.0913,  0.1499,  0.6009, -0.4814, -0.2563,\n",
      "           0.2409,  0.0426,  0.0963, -0.4804, -0.2223,  0.1422,  0.0072,\n",
      "           0.3366,  0.3170, -0.0832, -0.4803,  0.0890,  0.2696,  0.1111,\n",
      "           0.4598]]], grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "sentence = \"Deep learning is fascinating\"\n",
    "layer = 0  # Layer index\n",
    "head = 0  # Head index\n",
    "output = compute_output(sentence, layer, head)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Context Vectors with Persistent Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "\n",
    "def compute_distances_and_plot_barcode(output):\n",
    "    # Detach the output tensor, squeeze, and convert to numpy array\n",
    "    output_np = output.squeeze().detach().numpy()\n",
    "\n",
    "    # Compute the pairwise Euclidean distance matrix\n",
    "    distances = distance_matrix(output_np, output_np)\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "    \n",
    "    # Plot the barcode diagram\n",
    "    gd.plot_persistence_barcode(persistence=persistent_homology)\n",
    "    plt.show()\n",
    "\n",
    "# Test the function\n",
    "compute_distances_and_plot_barcode(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0c9b94b2654523bbe1bf6d74890a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='eps', max=1.4685055442168033, step=0.01), Output()),â€¦"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gudhi as gd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from scipy.spatial import distance_matrix\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Define the function to compute the output\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O, inputs\n",
    "\n",
    "# Compute the output\n",
    "output, inputs = compute_output(\"Deep learning is fascinating\", 0, 0)\n",
    "\n",
    "# Convert the output tensor to numpy array\n",
    "output_np = output.detach().numpy()[0]\n",
    "\n",
    "# Compute the pairwise Euclidean distance matrix\n",
    "distances = distance_matrix(output_np, output_np)\n",
    "\n",
    "# Compute the number of tokens\n",
    "num_tokens = len(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Decompose output to 3D for visualization\n",
    "# pca = PCA(n_components=3)\n",
    "# output_3d = pca.fit_transform(output_np)\n",
    "\n",
    "# Decompose output to 3D for visualization using t-SNE\n",
    "tsne = TSNE(n_components=3, perplexity=num_tokens-1)  # Set perplexity to the number of tokens\n",
    "output_3d = tsne.fit_transform(output_np)\n",
    "\n",
    "# Define the function to update the plot\n",
    "def update_plot(eps):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(output_3d[:, 0], output_3d[:, 1], output_3d[:, 2])\n",
    "    \n",
    "    # Add labels\n",
    "    for i, token_id in enumerate(inputs['input_ids'][0]):\n",
    "        ax.text(output_3d[i, 0], output_3d[i, 1], output_3d[i, 2], tokenizer.decode([token_id]))\n",
    "    \n",
    "    # Add edges\n",
    "    for i in range(distances.shape[0]):\n",
    "        for j in range(i+1, distances.shape[1]):\n",
    "            if distances[i, j] <= eps:\n",
    "                ax.plot([output_3d[i, 0], output_3d[j, 0]], \n",
    "                        [output_3d[i, 1], output_3d[j, 1]], \n",
    "                        [output_3d[i, 2], output_3d[j, 2]], 'b-')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create the interactive widget for the threshold distance parameter\n",
    "eps_slider = widgets.FloatSlider(min=0, max=np.max(distances), step=0.01, value=0)\n",
    "widgets.interactive(update_plot, eps=eps_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           0,
           1,
           2,
           3,
           4,
           5
          ],
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "size": 10
         },
         "mode": "markers+text",
         "text": [
          "[CLS]",
          "deep",
          "learning",
          "is",
          "fascinating",
          "[SEP]"
         ],
         "type": "scatter3d",
         "x": [
          -171.76446533203125,
          12.193901062011719,
          -91.67666625976562,
          144.9464569091797,
          11.148269653320312,
          57.02982711791992
         ],
         "y": [
          -42.68858337402344,
          87.39753723144531,
          -31.66512680053711,
          -83.59904479980469,
          114.6353759765625,
          67.42989349365234
         ],
         "z": [
          7.648690223693848,
          -119.53813171386719,
          325.1586608886719,
          29.715131759643555,
          144.9017791748047,
          -352.3066101074219
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          12.193901062011719,
          -171.76446533203125
         ],
         "y": [
          87.39753723144531,
          -42.68858337402344
         ],
         "z": [
          -119.53813171386719,
          7.648690223693848
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -91.67666625976562,
          -171.76446533203125
         ],
         "y": [
          -31.66512680053711,
          -42.68858337402344
         ],
         "z": [
          325.1586608886719,
          7.648690223693848
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          -91.67666625976562,
          12.193901062011719
         ],
         "y": [
          -31.66512680053711,
          87.39753723144531
         ],
         "z": [
          325.1586608886719,
          -119.53813171386719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          144.9464569091797,
          -171.76446533203125
         ],
         "y": [
          -83.59904479980469,
          -42.68858337402344
         ],
         "z": [
          29.715131759643555,
          7.648690223693848
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          144.9464569091797,
          12.193901062011719
         ],
         "y": [
          -83.59904479980469,
          87.39753723144531
         ],
         "z": [
          29.715131759643555,
          -119.53813171386719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          144.9464569091797,
          -91.67666625976562
         ],
         "y": [
          -83.59904479980469,
          -31.66512680053711
         ],
         "z": [
          29.715131759643555,
          325.1586608886719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          11.148269653320312,
          -171.76446533203125
         ],
         "y": [
          114.6353759765625,
          -42.68858337402344
         ],
         "z": [
          144.9017791748047,
          7.648690223693848
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          11.148269653320312,
          12.193901062011719
         ],
         "y": [
          114.6353759765625,
          87.39753723144531
         ],
         "z": [
          144.9017791748047,
          -119.53813171386719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          11.148269653320312,
          -91.67666625976562
         ],
         "y": [
          114.6353759765625,
          -31.66512680053711
         ],
         "z": [
          144.9017791748047,
          325.1586608886719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          11.148269653320312,
          144.9464569091797
         ],
         "y": [
          114.6353759765625,
          -83.59904479980469
         ],
         "z": [
          144.9017791748047,
          29.715131759643555
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          57.02982711791992,
          -171.76446533203125
         ],
         "y": [
          67.42989349365234,
          -42.68858337402344
         ],
         "z": [
          -352.3066101074219,
          7.648690223693848
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          57.02982711791992,
          12.193901062011719
         ],
         "y": [
          67.42989349365234,
          87.39753723144531
         ],
         "z": [
          -352.3066101074219,
          -119.53813171386719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          57.02982711791992,
          -91.67666625976562
         ],
         "y": [
          67.42989349365234,
          -31.66512680053711
         ],
         "z": [
          -352.3066101074219,
          325.1586608886719
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          57.02982711791992,
          144.9464569091797
         ],
         "y": [
          67.42989349365234,
          -83.59904479980469
         ],
         "z": [
          -352.3066101074219,
          29.715131759643555
         ]
        },
        {
         "line": {
          "color": "red",
          "width": 6
         },
         "mode": "lines",
         "type": "scatter3d",
         "x": [
          57.02982711791992,
          11.148269653320312
         ],
         "y": [
          67.42989349365234,
          114.6353759765625
         ],
         "z": [
          -352.3066101074219,
          144.9017791748047
         ]
        }
       ],
       "layout": {
        "scene": {
         "xaxis": {
          "title": {
           "text": "X"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75e22c637b4150b1ca11a90f0912b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.4785055442168034, description='eps', max=1.4785055442168034, step=0.â€¦"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets, HBox, Output\n",
    "import plotly as py\n",
    "from plotly.offline import iplot\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Define the function to compute the output\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O, inputs\n",
    "\n",
    "# Compute the output\n",
    "output, inputs = compute_output(\"Deep learning is fascinating\", 0, 0)\n",
    "\n",
    "# Convert the output tensor to numpy array\n",
    "output_np = output.detach().numpy()[0]\n",
    "\n",
    "# Compute the pairwise Euclidean distance matrix\n",
    "distances = distance_matrix(output_np, output_np)\n",
    "\n",
    "# Compute the number of tokens\n",
    "num_tokens = len(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Decompose output to 3D for visualization using t-SNE\n",
    "tsne = TSNE(n_components=3, perplexity=num_tokens-1)  # Set perplexity to the number of tokens\n",
    "output_3d = tsne.fit_transform(output_np)\n",
    "\n",
    "# Get token labels\n",
    "labels = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Define the figure for plotting\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define the function to update the plot\n",
    "def update_plot(eps):\n",
    "    fig.data = []  # clear the data\n",
    "\n",
    "    # Add points\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=output_3d[:, 0],\n",
    "            y=output_3d[:, 1],\n",
    "            z=output_3d[:, 2],\n",
    "            mode='markers+text',\n",
    "            text=labels,  # Add token labels\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=np.arange(len(output_3d)),  # Use indices as colors\n",
    "                colorscale='Viridis',\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filter out distances greater than the threshold\n",
    "    edges = distances < eps\n",
    "\n",
    "    # Create lines connecting the points that are closer than the threshold\n",
    "    lines = []\n",
    "    for i in range(len(output_3d)):\n",
    "        for j in range(i):\n",
    "            if edges[i, j]:  # If the distance is less than the threshold\n",
    "                lines.append(\n",
    "                    go.Scatter3d(\n",
    "                        x=[output_3d[i, 0], output_3d[j, 0]],\n",
    "                        y=[output_3d[i, 1], output_3d[j, 1]],\n",
    "                        z=[output_3d[i, 2], output_3d[j, 2]],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='red', width=6)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Add the lines to the figure\n",
    "    for line in lines:\n",
    "        fig.add_trace(line)\n",
    "\n",
    "    # Update the layout and display the figure\n",
    "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Create the interactive widget for the threshold distance parameter\n",
    "eps_slider = widgets.FloatSlider(min=0, max=np.max(distances)+0.01, step=0.01, value=np.max(distances)+0.01)\n",
    "widgets.interactive(update_plot, eps=eps_slider)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation and Keyphrase-Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use persistent homology of a single attention head's context vectors to perform collocation and keyword-keyphrase extraction as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = [\"Deep learning has revolutionized the field of artificial intelligence, offering innovative solutions to complex problems across various industries. Deep learning algorithms, a subset of machine learning, allow computers to learn from vast amounts of data. While a human brain can naturally perform this task, teaching a computer to understand and learn from data is a significant achievement in the realm of artificial intelligence. Deep learning models can identify patterns and reproduce human-like decision-making capabilities, which makes them incredibly valuable for various applications. For instance, in the healthcare sector, deep learning algorithms are being utilized for early detection of diseases and patient care management. Similarly, in the field of autonomous vehicles, deep learning plays a crucial role in enabling these vehicles to understand and interact with their surroundings. By processing and learning from large amounts of data, deep learning models can accurately predict and respond to different situations, thereby ensuring safety and efficiency. Moreover, deep learning is also being used in natural language processing to understand and generate human language. From voice assistants like Siri and Alexa to translation services like Google Translate, deep learning is at the core of these services, enabling them to understand and interact with users in a more human-like manner. Furthermore, deep learning is also paving the way for advancements in the field of image and video processing. From generating high-quality visual effects for movies to improving the quality of video calls, deep learning algorithms are enhancing the way we interact with digital media.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288, 64])\n",
      "tensor([[[ 0.0340,  0.0935, -0.0390,  ...,  0.0113, -0.0101,  0.0798],\n",
      "         [ 0.0722,  0.0933, -0.0438,  ...,  0.0252,  0.0075,  0.1362],\n",
      "         [ 0.3029,  0.5654, -0.1230,  ..., -0.3581,  0.3543,  0.0405],\n",
      "         ...,\n",
      "         [-0.1040, -0.0214, -0.0591,  ..., -0.1983,  0.0653, -0.1375],\n",
      "         [ 0.0095,  0.0694, -0.0408,  ..., -0.0356, -0.0215,  0.0417],\n",
      "         [ 0.0159,  0.0986, -0.0567,  ..., -0.0074,  0.0030,  0.0519]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O\n",
    "\n",
    "# Set the layer and head to use for computation\n",
    "layer = 5\n",
    "head = 10\n",
    "\n",
    "# Compute the context vectors for each text in the corpus\n",
    "context_2 = [compute_output(t, layer, head) for t in text_2]\n",
    "print(context_2[0].shape)\n",
    "print(context_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barcode for text 0:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "\n",
    "def compute_distances_and_plot_barcode(output):\n",
    "    # Detach the output tensor, squeeze, and convert to numpy array\n",
    "    output_np = output.squeeze().detach().numpy()\n",
    "\n",
    "    # Compute the pairwise Euclidean distance matrix\n",
    "    distances = distance_matrix(output_np, output_np)\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "    \n",
    "    # Plot the barcode diagram\n",
    "    gd.plot_persistence_barcode(persistence=persistent_homology)\n",
    "    plt.show()\n",
    "\n",
    "# Compute the barcode diagrams for each context vector\n",
    "for i, output in enumerate(context_2):\n",
    "    print(f\"Barcode for text {i}:\")\n",
    "    compute_distances_and_plot_barcode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for text 0:\n",
      "Cluster 0: [CLS], deep, artificial, intelligence, ., deep, while, human, naturally, artificial, intelligence, deep, human, incredibly, deep, autonomous, deep, by, deep, accurately, ., natural, language, from, deep, furthermore, deep, from, deep, digital, ., [SEP]\n",
      "Cluster 1: learning, algorithms\n",
      "Cluster 2: machine, learning\n",
      "Cluster 3: of, data\n",
      "Cluster 4: significant, achievement\n",
      "Cluster 5: patient, care\n",
      "Cluster 6: interact, with, surroundings\n",
      "Cluster 7: large, amounts\n",
      "Cluster 8: situations, ,\n",
      "Cluster 9: also, being\n",
      "Cluster 10: interact, with\n",
      "Cluster 11: interact, with\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def cluster_and_get_words(context, sentence, eps=0.85):\n",
    "    # Load pre-trained model tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Squeeze the context tensor to remove the batch size dimension\n",
    "    context = context.squeeze(0)\n",
    "\n",
    "    # Compute the pairwise distance matrix\n",
    "    distances = pdist(context.detach().numpy(), 'euclidean')\n",
    "    dist_matrix = squareform(distances)\n",
    "\n",
    "    # Run DBSCAN on the distance matrix\n",
    "    clustering = DBSCAN(eps=eps, min_samples=2, metric='precomputed').fit(dist_matrix)\n",
    "\n",
    "    # Get the words corresponding to each cluster\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(clustering.labels_):\n",
    "        if label != -1:  # Ignore noise (-1 label)\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(tokens[i])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Compute the clusters for each context vector\n",
    "clusters_2 = []\n",
    "for i, output in enumerate(context_2):\n",
    "    print(f\"Clusters for text {i}:\")\n",
    "    clusters_2.append(cluster_and_get_words(output, text_2[i]))\n",
    "    for label, words in clusters_2[-1].items():\n",
    "        print(f\"Cluster {label}: {', '.join(words)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288, 64])\n",
      "tensor([[[-0.0807,  0.0148,  0.0438,  ...,  0.0662, -0.1268,  0.0168],\n",
      "         [-0.0679,  0.0386,  0.0325,  ...,  0.0441, -0.1359,  0.0619],\n",
      "         [-0.0821,  0.0272,  0.0298,  ...,  0.0562, -0.1302,  0.0434],\n",
      "         ...,\n",
      "         [-0.4358,  0.3769, -0.1707,  ...,  0.3138,  0.6127, -0.3990],\n",
      "         [-0.0761,  0.0092,  0.0544,  ...,  0.0791, -0.0893, -0.0014],\n",
      "         [-0.0720,  0.0275,  0.0481,  ...,  0.0695, -0.1040,  0.0042]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Set the layer and head to use for computation\n",
    "layer = 7\n",
    "head = 9\n",
    "\n",
    "# Compute the context vectors for each text in the corpus\n",
    "context_3 = [compute_output(t, layer, head) for t in text_2]\n",
    "print(context_3[0].shape)\n",
    "print(context_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barcode for text 0:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the barcode diagrams for each context vector\n",
    "for i, output in enumerate(context_3):\n",
    "    print(f\"Barcode for text {i}:\")\n",
    "    compute_distances_and_plot_barcode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for text 0:\n",
      "Cluster 0: [CLS], deep, learning, has, artificial, intelligence, ,, across, ., deep, learning, algorithms, ,, a, subset, machine, learning, ,, allow, vast, ., while, a, human, brain, naturally, ,, teaching, a, ., deep, learning, models, can, identify, and, human, decision, making, ,, which, makes, incredibly, valuable, for, ., for, instance, ,, in, the, healthcare, sector, ,, deep, learning, algorithms, are, being, utilized, for, patient, ., similarly, ,, in, the, field, autonomous, ,, deep, learning, plays, these, vehicles, understand, interact, their, ., by, ,, deep, learning, models, can, accurately, ,, thereby, ., moreover, ,, deep, learning, is, also, ., from, voice, sir, translation, ,, understand, interact, ., furthermore, ,, deep, learning, is, also, paving, image, video, processing, ., from, to, video, ,, deep, learning, algorithms, digital, ., [SEP]\n",
      "Cluster 1: innovative, solutions\n",
      "Cluster 2: various, industries\n",
      "Cluster 3: understand, learn\n",
      "Cluster 4: significant, achievement, in\n",
      "Cluster 5: the, realm\n",
      "Cluster 6: artificial, intelligence\n",
      "Cluster 7: -, like, capabilities\n",
      "Cluster 8: various, applications\n",
      "Cluster 9: early, detection\n",
      "Cluster 10: a, crucial, role, in\n",
      "Cluster 11: processing, and\n",
      "Cluster 12: amounts, data\n",
      "Cluster 13: and, respond\n",
      "Cluster 14: to, different\n",
      "Cluster 15: safety, efficiency\n",
      "Cluster 16: being, used\n",
      "Cluster 17: understand, and, generate\n",
      "Cluster 18: assistants, like\n",
      "Cluster 19: ##i, and, alexa\n",
      "Cluster 20: services, like\n",
      "Cluster 21: deep, learning\n",
      "Cluster 22: the, core\n",
      "Cluster 23: a, more\n",
      "Cluster 24: the, for\n",
      "Cluster 25: high, quality, visual, effects\n"
     ]
    }
   ],
   "source": [
    "def cluster_and_get_words(context, sentence, eps=0.9):\n",
    "    # Load pre-trained model tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Squeeze the context tensor to remove the batch size dimension\n",
    "    context = context.squeeze(0)\n",
    "\n",
    "    # Compute the pairwise distance matrix\n",
    "    distances = pdist(context.detach().numpy(), 'euclidean')\n",
    "    dist_matrix = squareform(distances)\n",
    "\n",
    "    # Run DBSCAN on the distance matrix\n",
    "    clustering = DBSCAN(eps=eps, min_samples=2, metric='precomputed').fit(dist_matrix)\n",
    "\n",
    "    # Get the words corresponding to each cluster\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(clustering.labels_):\n",
    "        if label != -1:  # Ignore noise (-1 label)\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(tokens[i])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# Compute the clusters for each context vector\n",
    "clusters_3 = []\n",
    "for i, output in enumerate(context_3):\n",
    "    print(f\"Clusters for text {i}:\")\n",
    "    clusters_3.append(cluster_and_get_words(output, text_2[i]))\n",
    "    for label, words in clusters_3[-1].items():\n",
    "        print(f\"Cluster {label}: {', '.join(words)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point that we would like to illustrate is that using persistent homology to perform inform the parameters of a density based clustering algorithm like DBSCAN provides us a way to use individual attention heads to extract collocations and keyphrases of a text. The fact that an individual attention head can be used to find collocations and keyphrases is really fascinating! It also provides us with a way to compare different heads linguistically, as some heads do this better than others, and some tend to find certain collocations and keyphrases better than other collocations and keyphrases. So, we could potentially use this to better train models, or select which heads to apply Low Rank Adaptations (LoRAs) to in place of fine-tuning the entire model. In other words, if we train a LoRA to modify the behavior of individual attention heads so that they better detect certain collocations and keyphrases for different kinds of text corpora, we can get a more robust model overall. We might also use this to better find problematic knowledge that the model has learned and train this out of the model using a LoRA in place of fine tuning the entire model. We can test how each individual attention head behaves over a wide range of text corpora that is known to have certain kinds of problematic information in it, and deter the model from focusing on this kind of knowledge, treating its context vectors as \"noise\" (in terms of the DBSCAN). We can also use this to train models to detect problematic material in content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
