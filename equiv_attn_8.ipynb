{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Notes on Group Equivariant Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f \\in L_{\\mathbb{R}^d}(S)$, where $S$ is a finite set of indices, we define three functions:\n",
    "\n",
    "1. Key function $\\varphi_{key}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_k}}(S)$\n",
    "2. Query function $\\varphi_{query}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_k}}(S)$\n",
    "3. Value function $\\varphi_{value}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_v}}(S)$\n",
    "\n",
    "These functions are used to compute the self-attention for each element in $S$. The self-attention mechanism also incorporates a relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ that encodes the relative positions between elements.\n",
    "\n",
    "The self-attention function $\\alpha[f]: S \\times S \\to \\mathbb{R}$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\alpha[f](i, j) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f(i)), \\varphi_{key}(f(j)) + \\rho(i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f(i)), \\varphi_{key}(f(k)) + \\rho(i, k) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "Now, we want to extend the domain of these functions from $S$ to $\\mathcal{X}$ using the quotient space $\\mathcal{X} = G/\\mathscr{H}$, where $\\mathcal{X}$ is a homogeneous space. We have the coordinate function $x: S \\to \\mathcal{X}$ that maps elements of $S$ to $\\mathcal{X}$, and we can define $f_{\\mathcal{X}}: \\mathcal{X} \\to \\mathbb{R}^d$ such that $f_{\\mathcal{X}}(x(i)) = f(i)$. To extend the domain of the key, query, and value functions, we have:\n",
    "\n",
    "1. Key function $\\varphi_{key}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, where $\\varphi_{key}(f_{\\mathcal{X}}(x(i))) = \\varphi_{key}(f(i))$\n",
    "2. Query function $\\varphi_{query}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, where $\\varphi_{query}(f_{\\mathcal{X}}(x(i))) = \\varphi_{query}(f(i))$\n",
    "3. Value function $\\varphi_{value}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$, where $\\varphi_{value}(f_{\\mathcal{X}}(x(i))) = \\varphi_{value}(f(i))$\n",
    "\n",
    "\n",
    "In the context of self-attention, the query, key, and value functions play essential roles in transforming the input features. Each of these functions has a specific domain and codomain, as described below:\n",
    "\n",
    "1. Query function ($\\varphi_{qry}$):\n",
    "\n",
    "- Domain: The input space of the query function is the feature space associated with the elements of the input set, denoted as $L_{\\mathbb{R}^d}(S)$ for the original self-attention, and $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$. In the case of group self-attention, the domain is $L_{\\mathbb{R}^d}(G)$.\n",
    "\n",
    "- Codomain: The output space of the query function is a transformed feature space, typically of dimension $d_k$. It is denoted as $L_{\\mathbb{R}^{d_k}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_k}}(G)$ for group self-attention.\n",
    "\n",
    "2. Key function ($\\varphi_{key}$):\n",
    "\n",
    "- Domain: Similar to the query function, the input space of the key function is the feature space associated with the elements of the input set, denoted as $L_{\\mathbb{R}^d}(S)$ for the original self-attention, $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^d}(G)$ for group self-attention.\n",
    "\n",
    "- Codomain: The output space of the key function is also a transformed feature space, typically of dimension $d_k$. It is denoted as $L_{\\mathbb{R}^{d_k}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_k}}(G)$ for group self-attention.\n",
    "\n",
    "3. Value function ($\\varphi_{val}$):\n",
    "\n",
    "- Domain: The input space of the value function is the same as that of the query and key functions: $L_{\\mathbb{R}^d}(S)$ for the original self-attention, $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^d}(G)$ for group self-attention.\n",
    "\n",
    "- Codomain: The output space of the value function is a transformed feature space, typically of dimension $d_v$. It is denoted as $L_{\\mathbb{R}^{d_v}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_v}}(G)$ for group self-attention.\n",
    "\n",
    "These functions act on the input features and transform them into a suitable representation for computing self-attention, which allows the model to capture and utilize dependencies between different input elements.\n",
    "\n",
    "The notation $L_{\\mathbb{R}^d}(S)$, $L_{\\mathbb{R}^d}(\\mathcal{X})$, and $L_{\\mathbb{R}^d}(G)$ represent function spaces of functions mapping from the respective domain to the feature space $\\mathbb{R}^d$. Here, $d$ is the dimension of the feature space.\n",
    "\n",
    "1. $L_{\\mathbb{R}^d}(S)$: This represents the space of functions that map from the input set $S$ to the $d$-dimensional feature space $\\mathbb{R}^d$. In the context of self-attention, $S$ typically represents a sequence or a set of elements (e.g., words in a sentence, pixels in an image, or nodes in a graph), and the function maps each element in the set to a $d$-dimensional feature vector.\n",
    "\n",
    "2. $L_{\\mathbb{R}^d}(\\mathcal{X})$: This represents the space of functions that map from the homogeneous space $\\mathcal{X}$ to the $d$-dimensional feature space $\\mathbb{R}^d$. The homogeneous space $\\mathcal{X}$ is formed by the quotient space $G/\\mathscr{H}$, where $G$ is a group (e.g., $\\mathbb{R}^2 \\rtimes \\mathscr{H}$) and $\\mathscr{H}$ is a subgroup (e.g., $SO(2)$ or $SO(3)$). The functions in this space map each element in the homogeneous space $\\mathcal{X}$ to a $d$-dimensional feature vector.\n",
    "\n",
    "3. $L_{\\mathbb{R}^d}(G)$: This represents the space of functions that map from the group $G$ to the $d$-dimensional feature space $\\mathbb{R}^d$. In the context of group self-attention, $G$ is a group acting on the input set (e.g., translation or rotation group), and the functions in this space map each element in the group $G$ to a $d$-dimensional feature vector.\n",
    "\n",
    "For the codomains, the notations $L_{\\mathbb{R}^{d_k}}(S)$, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, $L_{\\mathbb{R}^{d_k}}(G)$, $L_{\\mathbb{R}^{d_v}}(S)$, $L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$, and $L_{\\mathbb{R}^{d_v}}(G)$ are similar but refer to the output spaces of the respective query, key, and value functions. The output spaces are also feature spaces but with different dimensions, such as $d_k$ for the query and key functions and $d_v$ for the value function.\n",
    "\n",
    "\n",
    "Next, we need to extend the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}$. To extend the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ to the domain $\\mathcal{X} \\times \\mathcal{X}$, we define a new function $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ such that $\\rho^P(x(j) - x(i)) = \\rho(i, j)$. \n",
    "\n",
    "Now, we can define the extended self-attention function $\\alpha[f_{\\mathcal{X}}]: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ as follows:\n",
    "\n",
    "$$\n",
    "\\alpha[f_{\\mathcal{X}}](x(i), x(j)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(j))) + \\rho^P(x(j)-x(i)) \\rangle\\right)}{\\sum_{x(k) \\in \\mathcal{X}}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(k))) + \\rho^P(x(k)-x(i)) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "By extending the domain of the key, query, and value functions and the relative positional encoding to $\\mathcal{X}$, we have successfully extended the self-attention mechanism from a finite set $S$ to a homogeneous space $\\mathcal{X}$. This lays the groundwork for further generalizing the self-attention mechanism to incorporate the structure of the group $G$ and develop group self-attention using liftings. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifting Self-Attention and Group Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us proceed by explaining how to lift the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ by first extending its domain to $\\mathcal{X} \\times \\mathcal{X}$ and then defining $\\mathcal{L}[\\rho](i, j) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))$. This can be thought of as first extending the domain of $\\rho$ to $\\mathcal{X} \\times \\mathcal{X}$, then lifting $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ to $\\mathcal{L}[\\rho]: G \\to \\mathbb{R}^d$. To lift the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ to incorporate the group structure $G$, we follow these steps:\n",
    "\n",
    "1. Extend the domain of $\\rho$ to $\\mathcal{X} \\times \\mathcal{X}$ by defining a new function $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ such that $\\rho^P(x(j) - x(i)) = \\rho(i, j)$. \n",
    "\n",
    "2. Define a lifting function $\\mathcal{L}_h[\\rho]: S \\times S \\to \\mathbb{R}^d$ that incorporates the action of the group element $h \\in G$ on the relative positional encoding. The lifting function is defined as follows:\n",
    "\n",
    "$$\n",
    "L_h[\\rho](i, j) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))\n",
    "$$\n",
    "\n",
    "Here, $h^{-1}$ denotes the inverse of the group element $h$. The lifting function essentially \"lifts\" the relative positional encoding to the group by applying the group action on the elements of $\\mathcal{X}$ before computing the relative positional encoding.\n",
    "\n",
    "\n",
    "To represent the lifted relative positional encoding as the concatenation of encodings indexed by $h \\in \\mathscr{H}$, we first define the lifted relative positional encoding for each element $h \\in \\mathscr{H}$ as follows:\n",
    "\n",
    "$$\n",
    "L_{h}[\\rho](x(i), x(j)) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))\n",
    "$$\n",
    "\n",
    "Now, we can concatenate the lifted relative positional encodings for all elements in the subgroup $\\mathscr{H}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}[\\rho](x(i), x(j)) = \\{ L_{h}[\\rho](x(i), x(j)) \\big\\}_{h \\in \\mathscr{H}}\n",
    "$$\n",
    "\n",
    "Now, we can define a lifted self-attention mechanism that incorporates the group structure. The lifted self-attention function $\\alpha_{(h)}[f_{\\mathcal{X}}]: S \\times S \\to \\mathbb{R}$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\alpha_{(h)}[f_{\\mathcal{X}}](i, j) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(j))) + L_h[\\rho](i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(k))) + L_h[\\rho](i, k) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "By using the lifted self-attention function $\\alpha_{(h)}[f_{\\mathcal{X}}]$, we can now capture the dependencies between input elements while taking into account the group structure $G$. In particular, we concatenate multiple lifted self-attention operations, indexed by $h \\in \\mathscr{H}$ to obtain $\\{\\alpha_{(h)}[f_{\\mathcal{X}}]\\}_{h \\in \\mathscr{H}}$. Moreover, we can write\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_{(h)}[f_{\\mathcal{X}}](i, j) &= \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}((i, e)))), \\varphi_{key}(f_{\\mathcal{X}}((j, e))) + L_h[\\rho](i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}((i, e))), \\varphi_{key}(f_{\\mathcal{X}}((k, e))) + L_h[\\rho](i, k) \\rangle\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "where $e \\in \\mathscr{H}$ is the identity element. This generalization of the self-attention mechanism allows for more expressive modeling of complex data structures and can be particularly useful when the input data have a natural group structure, such as images with translation and rotation invariance or graphs with symmetries. Now, we would like to define the group self-attention map as \n",
    "\n",
    "$$\n",
    "\\alpha[f_G]((i, h_1), (j, h_2)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}^G(f_G(i, h_1)), \\varphi_{key}^G(f_G(j, h_2) + L_h[\\rho]((i,h_1), (j, h_2))) \\rangle\\right)}{\\sum_{(k, h_3) \\in N(i, h_1) \\subset G}\\exp\\left(\\langle \\varphi_{qry}^G(f_G(i, h_1)), \\varphi_{key}^G(f_G(k, h_3) + L_h[\\rho]((i, h_1),(k, h_3))) \\rangle\\right)},\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define \n",
    "\n",
    "$$\n",
    "L_h[\\rho]((i, h_1), (j, h_2)) = \\rho^P(h^{-1}(x(j) - x(i)), h^{-1}(h_1^{-1} h_2))\n",
    "$$\n",
    "\n",
    "To generalize the group self-attention to multihead group self-attention, we will incorporate multiple attention heads, each with its own key, query, and value functions. This allows the model to capture different aspects of the input data by combining the attention weights from each head.\n",
    "\n",
    "Let $[H]$ be the number of attention heads. We will denote the key, query, and value functions for each head as $\\varphi_{key}^{head}, \\varphi_{qry}^{head},$ and $\\varphi_{val}^{head}$, respectively. Now, we can define the multihead group self-attention function $\\alpha^{head}[f_G]: G \\times G \\to \\mathbb{R}$ for each head as:\n",
    "\n",
    "$$\n",
    "\\alpha^{head}[f_G]((i, h_1), (j, h_2)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(i, h_1)), \\varphi_{key}^{head}(f_G(j, h_2)) + L_h [\\rho]((i, h_1), (j, h_2)) \\rangle\\right)}{\\sum_{(k, h_3) \\in N(i, h_1) \\subset G}\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(i, h_1)), \\varphi_{key}^{head}(f_G(k, h_3)) + L_h [\\rho]((i, h_1), (k, h_3)) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "For each head, we compute the corresponding attention weights and apply them to the value function, yielding a weighted sum of the value vectors. We then concatenate the results from all heads and apply an output function $\\varphi_{out}$:\n",
    "\n",
    "$$\n",
    "m_G^r[f, \\rho](i, h) = \\varphi_{out}\\left( \\bigcup_{head \\in [H]} \\sum_{h_1 \\in \\mathscr{H}} \\sum_{(j, h_2) \\in N(i,h_1)} \\alpha^{head}[f_G]((i, h_1), (j, h_2)) \\varphi_{val}^{head}(f_G(j, h_2)) \\right)\n",
    "$$\n",
    "\n",
    "This multihead group self-attention mechanism captures multiple relationships in the input data, allowing the model to better understand and represent the underlying structure. By incorporating the structure of the group $G$ and its action on the homogeneous space $\\mathcal{X}$, multihead group self-attention provides a powerful and flexible way to model data with geometric and topological structures.\n",
    "\n",
    "To reiterate, let $G = \\mathcal{X} \\rtimes \\mathscr{H}$ be an affine group acting on itself, and $f(g) = f(i, \\tilde{h}) \\in L_{\\mathbb{R}^d}(G)$, $i \\in S$, $\\tilde{h} \\in \\mathscr{H}$, be a function defined on a set immersed with the structure of the group $G$. That is, enriched with a positional encoding $\\rho((i, \\tilde{h}), (j, \\hat{h})) := \\rho^P((x(j) - x(i), \\tilde{h}^{-1} \\hat{h})), i, j \\in S, \\tilde{h}, \\hat{h} \\in \\mathscr{H}$. The group self-attention $m^r_G[f, \\rho] : L_{\\mathbb{R}^d}(G) \\to L_{\\mathbb{R}^d}(G)$ is a map from functions on $G$ to functions on $G$ obtained by modifying the group positional encoding by the action of group elements $h \\in \\mathscr{H}$: $\\{L_h [\\rho]((i, \\tilde{h}), (j, \\hat{h}))\\}_{h \\in \\mathscr{H}}$, $L_h [\\rho]((i, \\tilde{h}), (j, \\hat{h})) = \\rho_P(h^{-1}(x(j) - x(i)), h^{-1}(\\tilde{h}^{-1} \\hat{h}))$. It corresponds to the concatenation of multiple self-attention operations (Eq. 11) indexed by $h$ with varying positional encodings $L_h [\\rho]$ and followed by a summation over the output domain along $\\tilde{h}$:\n",
    "\n",
    "$$\n",
    "m^r_G[f, \\rho](i, h) = \\sum_{\\tilde{h} \\in \\mathscr{H}} m^r[f, L_h[\\rho]](i, \\tilde{h}) \\hspace{1cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\varphi_{\\text{out}} \\Bigg( \\bigcup_{h \\in [H]} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j, \\hat{h}) \\in N(i, \\tilde{h})} \\sigma_{j, \\hat{h}} \\Big\\langle \\varphi^{(h)}_{\\text{qry}}(f(i, \\tilde{h})), \\varphi^{(h)}_{\\text{key}}(f(j, \\hat{h})) + L_h[\\rho]((i, \\tilde{h}), (j, \\hat{h})) \\Big\\rangle \\varphi^{(h)}_{\\text{val}}(f(j, \\hat{h})) \\Bigg).\n",
    "$$\n",
    "\n",
    "Now, we can also rewrite this as follows. Given in terms of elements $g \\in G$ instead of as pairs of the form $(x, h)$, let $g_1 = (x_1, h_1)$, $g_2 = (x_2, h_2)$, and $g_3 = (x_3, h_3)$ be elements of the semi-direct product $G = \\mathcal{X} \\rtimes \\mathscr{H}$, where $g_1, g_2, g_3 \\in G$. Then, we can rewrite the formula as follows:\n",
    "\n",
    "$$\n",
    "\\alpha^{head}[f_G](g_1, g_2) = \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(g_1)), \\varphi_{key}^{head}(f_G(g_2) + L_h [\\rho](g_1, g_2)) \\rangle\\right)}{\\sum_{g_3 \\in N(g_1) \\subset G}\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(g_1)), \\varphi_{key}^{head}(f_G(g_3) + L_h [\\rho](g_1, g_3)) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "In this rewritten formula, we have replaced the pairs of the form $(x, h)$ with the corresponding elements $g \\in G$. The formula still represents the self-attention function, but it is now expressed directly in terms of elements of the group $G$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the given equations more readable and maintain the integrity of the content, I have added line breaks to the long lines. he following is the calculations necessary for proving that group self-atention is equivariant, \n",
    "\n",
    "\\begin{align*}\n",
    "m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j,\\hat{h}) \\in N(i, \\tilde{h})} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (L_y L_{\\bar{h}}[f](i, \\tilde{h})), \\varphi^{head}_{\\text{key}} (L_y L_{\\bar{h}}[f](j, \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(j, \\hat{h}))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(i, \\tilde{h})} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (L_y L_{\\bar{h}}[f](i, \\tilde{h})), \\varphi^{head}_{\\text{key}} (L_y L_{\\bar{h}}[f](k, \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(k, \\hat{h}))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (L_y L_{\\bar{h}}[f](j, \\hat{h})) \\\\\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j,\\hat{h}) \\in N(i, \\tilde{h})} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1} \\tilde{h})), \\varphi^{head}_{\\text{key}} (f(x^{-1}(\\bar{h}^{-1}(x(j) - y)), \\bar{h}^{-1} \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(j, \\hat{h}))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(i, \\tilde{h})} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1} \\tilde{h})), \\varphi^{head}_{\\text{key}} (f(x^{-1}(\\bar{h}^{-1}(x(k) - y)), \\bar{h}^{-1} \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(k, \\hat{h}))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (f(x^{-1}(\\bar{h}^{-1}(x(j) - y)), \\bar{h}^{-1} \\hat{h})) \\\\\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(\\bar{i}, \\tilde{h}')), \\varphi^{head}_{\\text{key}} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_h [\\rho]((x^{-1}(\\bar{h}x(\\bar{i}) + y), \\bar{h} \\tilde{h}'),\n",
    "(x^{-1}(\\bar{h}x(\\bar{j}) + y), \\bar{h} \\hat{h}'))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(\\bar{i}, \\tilde{h}')), \\varphi^{head}_{\\text{key}} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_h [\\rho]((x^{-1}(\\bar{h}x(\\bar{i}) + y), \\bar{h} \\tilde{h}'),\n",
    "(x^{-1}(\\bar{h}x(\\bar{j}) + y), \\bar{h} \\hat{h}'))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)\n",
    "&= \\phi_{out} \\sum_{h \\in \\mathscr{H}} \\sum_{\\bar{h}\\tilde{h}' \\in \\mathscr{H}}\\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ \\rho_P(h^{-1}\\bar{h}(x(\\bar{j}) - x(\\bar{i}), \\tilde{h}'^{-1} \\hat{h}'))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ \\rho_P(h^{-1}\\bar{h}(x(\\bar{j}) - x(\\bar{i}), \\tilde{h}'^{-1} \\hat{h}'))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi_{val}^{head} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "&= \\phi_{out} \\sum_{h \\in \\mathscr{H}} \\sum_{\\bar{h}\\tilde{h}' \\in \\mathscr{H}} \\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_{\\bar{h}^{-1}h}[\\rho]((\\bar{i}, \\tilde{h}'),(\\bar{j}, \\hat{h}')))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_{\\bar{h}^{-1}h}[\\rho]((\\bar{i}, \\tilde{h}'),(\\bar{j}, \\hat{h}')))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi_{val}^{head} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "&= m_G^r[f, \\rho](\\bar{i}, \\bar{h}^{-1}h)\\\\\n",
    "&= m_G^r[f, \\rho](x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1}h) \\\\\n",
    "&= L_y L_{\\bar{h}}[m_G^r[f, \\rho]](i,h).\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equations describe the process of applying the multihead group self-attention mechanism in a geometric context. The main goal is to show that the multihead group self-attention operation commutes with the left action of the group $G$ on the homogeneous space $\\mathcal{X}$.\n",
    "\n",
    "The first set of equations starts by expressing the multihead group self-attention function $m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)$ for an input function $f$. It uses the output function $\\phi_{\\text{out}}$, the key, query, and value functions $\\varphi^{head}_{\\text{key}}$, $\\varphi^{head}_{\\text{qry}}$, and $\\varphi^{head}_{\\text{val}}$, respectively, and the group action $L_h [\\rho]$.\n",
    "\n",
    "The second and third lines of the first set of equations substitute the lifted functions $L_y L_{\\bar{h}}[f]$ for the original function $f$. This shows the transformation of indices with the group action.\n",
    "\n",
    "The fourth and fifth lines of the first set of equations rewrite the terms using the inverse group action $\\bar{h}^{-1}$ and the group action on the homogeneous space $\\mathcal{X}$.\n",
    "\n",
    "The second set of equations starts by repeating the expression for the multihead group self-attention function $m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)$. \n",
    "\n",
    "The following lines of the second set of equations show that the multihead group self-attention operation $m_G^r[f, \\rho]$ commutes with the left action of the group $G$ on the homogeneous space $\\mathcal{X}$. This means that applying the group action before or after the multihead group self-attention operation yields the same result.\n",
    "\n",
    "In summary, these equations present the derivation that shows the commutativity property of the multihead group self-attention mechanism in a geometric context, which is important for understanding how this attention mechanism can be applied to model data with geometric and topological structures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Graphs Embedded in Surfaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the formalism of dessin d'enfants because it provides a unified mathematical framework for tokenizing complicated graphs embedded in Riemann surfaces like multi-holed tori. For an introduction, please see [Graphs on Surfaces and Their Applications](https://www-users.cse.umn.edu/~reiner/Classes/Math8680Fall2014Papers/LandoZvonkin.pdf). Let $[\\sigma, \\alpha]$ be a rotation system with $\\sigma \\in S_{2n}$ a permutation giving the vertices of the graph, attaches to half-edges. Let $\\alpha \\in S_{2n}$ be a permutation that is an involution, that is $\\alpha^2 = \\alpha$, which tells use how to glue the half-edges around the \"stars\" associated to each cycle of $\\sigma$. The involution $\\alpha$ can be thought of as making the graph a bipartite graph, by labeling each full edge by a white \"edge node\" that represents one of the two-cycles of $\\alpha$. We can then color the vertex nodes of $\\sigma$ black, and the edge nodes of $\\alpha$ white, making the graph bipartite. Now, the dual graph can be computed as follows. We replace $\\sigma$ with $\\phi = (\\sigma \\alpha)^{-1}$ and forms the dual dessin $[\\phi, \\alpha]$. This corresponds to adding in face nodes according to $\\phi$, each with half-edges that are connected according to $\\alpha$. There is a node on every face, and the edges connect neighboring faces that share an edge on the boundary. This again makes $[\\phi, \\alpha]$ a bipartite, cellularly embedded graph in the Riemann surface $\\Sigma$. It is the geometric dual graph of $[\\sigma, \\alpha]$. \n",
    "\n",
    "Now, we would like to tokenize any such graph, treating vertex, edge, and face nodes as tokens, and given a vector embedding $X$, which can ve thought of as a functions $\\Phi:\\Gamma \\to \\mathbb{R}^d$, where $\\Gamma$ is the graph obtained by overlaying $[\\sigma, \\alpha]$ and $[\\phi, \\alpha]$ in the surface $\\Sigma$. We can restrict $\\Phi \\in L_{\\mathbb{R}^d}(\\Gamma)$ to the vertices $\\Phi_V: V \\to \\mathbb{R}^d$ giving vertex feature vectors $\\Phi(v) \\in \\mathbb{R}^d$ for $v \\in V$ a vertex. Similarly, we can restrict $\\Phi_E: E \\to \\mathbb{R}^d$ giving edge feature vectors $\\Phi(e) \\in \\mathbb{R}^d$ for each edge $e \\in E$. Finally, we can also restrict $\\Phi_F: F \\to \\mathbb{R}^d$ giving a function $\\Phi_F \\in L_{\\mathbb{R}^d}(F)$, where $\\Phi(f) \\in \\mathbb{R}^d$ for each face $f \\in F$. Now, restriction gives us three functions \n",
    "\n",
    "- $\\Phi_V \\in L_{\\mathbb{R}^d}(V) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "- $\\Phi_E \\in L_{\\mathbb{R}^d}(E) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "- $\\Phi_F \\in L_{\\mathbb{R}^d}(F) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "\n",
    "and we have $\\Phi \\in L_{\\mathbb{R}^d}(\\Gamma)$ where $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$, where $\\Gamma_1$ is the orginial graph embedded in the Riemann surface according to $[\\sigma, \\alpha]$ and $\\Gamma_2 = \\Gamma_1^{\\vee}$ is the geometric dual graph of $\\Gamma_1$ and ided n the surface according to $[\\phi, \\alpha]$. With the feature function $\\Phi$ defined on $\\Gamma$ in hand, we can define three projection maps, \n",
    "\n",
    "\n",
    "- $\\varphi_{qry}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_k}}(\\Gamma)$\n",
    "- $\\varphi_{key}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_k}}(\\Gamma)$\n",
    "- $\\varphi_{val}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_v}}(\\Gamma)$\n",
    "\n",
    "\n",
    "and we can define attention as before. With the feature function $\\Phi$ defined on $\\Gamma$ and the projection maps $\\varphi_{qry}$, $\\varphi_{key}$, and $\\varphi_{val}$ in hand, we can now define the attention mechanism for the graph embedded in the Riemann surface $\\Sigma$. We can extend the previously defined multihead self-attention mechanism to work on the cellularly embedded graph $\\Gamma$ by considering the three types of nodes: vertex (V), edge (E), and face (F) nodes.\n",
    "\n",
    "Let's define the self-attention function for each node type as:\n",
    "\n",
    "1. $\\alpha[f_{\\mathcal{X}}](x(i), x(j))$\n",
    "\n",
    "For each node type, we compute the self-attention as follows:\n",
    "\n",
    "$$\n",
    "\\text{SA}^{head}[f_{\\mathcal{X}}](x(i), x(j)) = \\sum_{j \\in N(i) \\subseteq S}\\alpha^{head}[f_{\\mathcal{X}}](x(i), x(j)) \\cdot \\varphi_{val}^{head}(f_{\\mathcal{X}}(x(j))) \n",
    "$$\n",
    "\n",
    "Using the $\\bigcup_{h \\in [H]}$ notation and applying the function $\\phi_{out}$, the full multi-head self-attention formula can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MHA}[f_{\\mathcal{X}}](x(i)) = \\phi_{out}\\left(\\bigcup_{h \\in [H]}\\left(\\sum_{j \\in N(i) \\subseteq S} \\alpha^h[f_{\\mathcal{X}}](x(i), x(j)) \\cdot \\varphi_{val}^h(f_{\\mathcal{X}}(x(j)))\\right)\\right)\n",
    "$$\n",
    "\n",
    "In this formula, we compute the self-attention values $\\alpha^h[f_{\\mathcal{X}}](x(i), x(j))$ and the corresponding values from the value projection $\\varphi_{val}^h(f_{\\mathcal{X}}(x(j)))$ for each attention head $h$. We sum these values over all neighbors $j \\in N(i) \\subseteq S$, then concatenate the results from all attention heads in the set $[H]$. Finally, we apply the output function $\\phi_{out}$ to the concatenated result to obtain the final multi-head self-attention output for the node $x(i)$.\n",
    "\n",
    "Applying attention mechanisms to graphs embedded in Riemann surfaces, such as the torus code, requires positional encodings that can capture the topology and geometry of the graph and its dual. There are several types of positional encodings that might be appropriate for this task. Let's consider the options you've mentioned:\n",
    "\n",
    "1. Graph Laplacian positional encodings: Graph Laplacian positional encodings can capture the local connectivity of a graph, including its dual, and can be used to encode the position of nodes relative to each other. They are computed using the graph Laplacian matrix, which considers the connectivity of the graph. These encodings can capture local information and might be a suitable choice for the attention mechanism applied to the tokenization of a graph and its dual. However, they might not fully capture the global structure of the graph and its embedding in a Riemann surface.\n",
    "\n",
    "2. Relative positional encodings: Relative positional encodings are designed to capture the relative position of elements in a sequence or structure, making them suitable for encoding the position of nodes in the torus code. Since the torus code is essentially an image pixel grid on the torus, and relative positional encodings have been shown to work well for images, they could be an appropriate choice. However, it is crucial to adapt the relative positional encodings to the graph setting and take into account the specific structure of the torus code, including vertex, edge, and face nodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Tokenizing Graphs \n",
    "\n",
    "Following the ideas of [Pure Transformers are Powerful Graph Learners](https://arxiv.org/abs/2207.02505), we tokenize vertex nodes, edges nodes, (and face nodes if we are tokenizing dessins d'enfants) on equal footing, similar to the following image, except there is a \"third type\" idenifier in addition to \"$v$\" and \"$e\"$, denoyted by \"$f$\". \n",
    "\n",
    "<img src=\"/Users/amelieschreiber/vscode_projects/equivariant_attention/tokenizing_graphs.png\"  width=\"800\" height=\"500\">\n",
    "\n",
    "### Transformers for Graph Learning Tasks\n",
    "\n",
    "Transformers, originally developed for natural language processing tasks, have shown exceptional performance in various domains. In this section, we explore the adaptation and application of Transformers to graph learning tasks, delving into the challenges and techniques for representing graph-structured data in a format suitable for the Transformer architecture. We present a mathematically rigorous overview of a novel approach, the \\textit{Tokenized Graph Transformer} (TokenGT), which demonstrates the effectiveness of Transformers for processing graphs and outperforming traditional Graph Neural Networks (GNNs).\n",
    "\n",
    "#### Preliminaries\n",
    "\n",
    "Let $G = (V, E)$ be an input graph, where $V = \\{v_1, \\dots, v_n\\}$ is the set of $n$ nodes, and $E = \\{(u, v) \\mid u, v \\in V \\}$ is the set of $m$ edges. Each node $v_i \\in V$ and edge $e_j \\in E$ are associated with feature vectors $\\mathbf{x}_{v_i} \\in \\mathbb{R}^C$ and $\\mathbf{x}_{e_j} \\in \\mathbb{R}^C$, respectively. The goal is to learn a function $f: G \\rightarrow Y$, which maps the input graph $G$ to an output $Y$, using a Transformer architecture.\n",
    "\n",
    "#### Tokenizing Graphs\n",
    "\n",
    "A key challenge in adapting Transformers to process graphs is representing graph-structured data in a manner compatible with the Transformer architecture. To this end, we tokenize the graph by treating each node and edge as a separate token. The feature matrix $\\mathbf{X} \\in \\mathbb{R}^{(n+m) \\times C}$ is constructed by concatenating the node and edge feature matrices, $\\mathbf{X} = [\\mathbf{X}_V ; \\mathbf{X}_E]$.\n",
    "\n",
    "However, naively providing the token features $\\mathbf{X}$ as input to a Transformer would discard graph connectivity. To address this, we augment the tokens with token-wise embeddings consisting of orthonormal node identifiers and trainable type identifiers.\n",
    "\n",
    "#### Node Identifiers\n",
    "\n",
    "Given the input graph $G$, we first generate $n$ node-wise orthonormal vectors $\\mathbf{P} \\in \\mathbb{R}^{n \\times d_p}$ as node identifiers. The tokens are then augmented as follows:\n",
    "\n",
    "- For each node $v \\in V$, augment the token $\\mathbf{x}_v$ as $\\mathbf{x}_v' = [\\mathbf{x}_v , \\mathbf{p}_v , \\mathbf{p}_v]$.\n",
    "- For each edge $(u, v) \\in E$, augment the token $\\mathbf{x}_{(u,v)}$ as $\\mathbf{x}_{(u,v)}' = [\\mathbf{x}_{(u,v)}, \\mathbf{p}_u, \\mathbf{p}_v]$.\n",
    "\n",
    "The node identifiers enable the Transformer to capture the graph's connectivity structure by comparing the node identifiers between pairs of tokens.\n",
    "\n",
    "#### Type Identifiers\n",
    "\n",
    "To differentiate between node and edge tokens, we introduce trainable type identifiers. Let $\\mathbf{E} = [\\mathbf{e}_V ; \\mathbf{e}_E] \\in \\mathbb{R}^{2 \\times d_e}$ be a parameter matrix containing type identifiers $\\mathbf{e}_V$ for nodes and $\\mathbf{e}_E$ for edges. The tokens are further augmented as follows:\n",
    "\n",
    "- For each node $v \\in V$, augment the token $\\mathbf{x}_v'$ as $\\mathbf{x}_v'' = [\\mathbf{x}_v', \\mathbf{e}_V]$.\n",
    "- For each edge $(u, v) \\in E$, augment the token $\\mathbf{x}_{(u,v)}'$ as $\\mathbf{x}_{(u,v)}'' = [\\mathbf{x}_{(u,v)}', \\mathbf{e}_E]$.\n",
    "\n",
    "These type identifiers encode whether a token corresponds to a node or an edge, which is crucial for the Transformer to properly attend to the relevant tokens based on their roles in the graph.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Gamma_1$ be a graph embedded in a Riemann surface $\\Sigma$ according to the rotation system $[\\sigma, \\alpha]$, where $\\sigma \\in S_{2n}$ is a permutation representing the cyclic order of edges around each vertex, and $\\alpha \\in S_{2n}$ is an involution representing the gluing of half-edges around vertices. Let $\\Gamma_2 = \\Gamma_1^{\\vee}$ be the geometric dual graph of $\\Gamma_1$, also embedded in $\\Sigma$ according to the rotation system $[\\phi, \\alpha]$.\n",
    "\n",
    "We define a new graph $\\Gamma$ as the union of the two graphs, $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$, where each vertex, edge, and face of the embedded graphs are represented by nodes in $\\Gamma$. We will label each node in $\\Gamma$ with five labels: \"tail1\", \"head1\", \"tail2\", \"head2\", and \"type\".\n",
    "\n",
    "To rigorously describe the labeling process, let $V$, $E$, and $F$ represent the sets of vertices, edges, and faces, respectively, of the graphs $\\Gamma_1$ and $\\Gamma_2$. For each node $x \\in \\Gamma$, we define the labels as follows:\n",
    "\n",
    "1. If $x \\in V$, then the node represents a vertex of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the index of the vertex, denoted as $i_v(x)$. Similarly, we set the labels \"tail2\" and \"head2\" equal to $i_v(x)$ as well. The \"type\" label is set to \"$v$\".\n",
    "\n",
    "2. If $x \\in F$, then the node represents a face of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the index of the face, denoted as $i_f(x)$. Likewise, we set the labels \"tail2\" and \"head2\" equal to $i_f(x)$. The \"type\" label is set to \"$f$\".\n",
    "\n",
    "3. If $x \\in E$, then the node represents an edge of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the indices of the vertex nodes connected by the edge, denoted as $i_v^t(x)$ and $i_v^h(x)$, respectively. We set the labels \"tail2\" and \"head2\" equal to the indices of the face nodes connected by the edge, denoted as $i_f^t(x)$ and $i_f^h(x)$, respectively. The \"type\" label is set to \"$e$\".\n",
    "\n",
    "In summary, we have defined a graph $\\Gamma$ as the union of two embedded graphs $\\Gamma_1$ and $\\Gamma_2$, with each node in $\\Gamma$ representing either a vertex, edge, or face of the original graphs. We assign five labels to each node in $\\Gamma$ according to its role in the structure of the embedded graphs, providing a rigorous and clear description suitable for inclusion in a research article on deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Graph grammars provide a formal framework for representing, analyzing, and manipulating graph structures based on a set of rules that define how graphs can be constructed and transformed. They have been used in various domains, including computer science, linguistics, and computational biology, as a powerful tool for modeling complex structures and processes. In this introduction, we present the fundamental concepts of graph grammars, illustrate examples, and discuss their connections to modeling language, while maintaining mathematical rigor.\n",
    "\n",
    "**Preliminaries**\n",
    "\n",
    "Let $G = (V, E, \\lambda)$ be a labeled graph, where $V$ is the set of vertices, $E \\subseteq V \\times V$ is the set of edges, and $\\lambda: V \\cup E \\to \\Sigma$ is a labeling function that assigns labels from a finite alphabet $\\Sigma$ to the vertices and edges of the graph.\n",
    "\n",
    "A graph grammar consists of a set of rules that specify how graphs can be constructed and transformed. There are several types of graph grammars, including context-free graph grammars (CFGGs) and context-sensitive graph grammars (CSGGs), which differ in the complexity of the rules they employ and the expressiveness of the graph languages they can define.\n",
    "\n",
    "**Context-Free Graph Grammars (CFGGs)**\n",
    "\n",
    "A context-free graph grammar is a tuple $G = (N, T, P, S)$, where:\n",
    "\n",
    "- $N$ is a finite set of nonterminal symbols;\n",
    "- $T$ is a finite set of terminal symbols, with $N \\cap T = \\emptyset$;\n",
    "- $P$ is a finite set of production rules of the form $A \\to \\alpha$, where $A \\in N$ is a nonterminal symbol, and $\\alpha$ is a graph with vertices and edges labeled from $N \\cup T$;\n",
    "- $S \\in N$ is the start symbol.\n",
    "\n",
    "A derivation in a CFGG is a sequence of graph transformations that starts from the graph consisting of a single nonterminal vertex labeled with the start symbol $S$ and successively applies production rules to replace nonterminal vertices with graphs according to the rules in $P$. The language generated by a CFGG is the set of all graphs that can be derived from the start symbol.\n",
    "\n",
    "**Example 1**: Consider a simple CFGG with $N = \\{A\\}$, $T = \\{a, b\\}$, and production rules $P = \\{A \\to G_1, A \\to G_2\\}$, where $G_1$ is a graph with a single vertex labeled $a$ and $G_2$ is a graph with two vertices labeled $a$ and $b$ connected by an edge labeled $a$. The language generated by this grammar includes graphs with any number of vertices labeled $a$ and $b$, connected by edges labeled $a$.\n",
    "\n",
    "**Context-Sensitive Graph Grammars (CSGGs)**\n",
    "\n",
    "A context-sensitive graph grammar is a tuple $G = (N, T, P, S)$, where $N$, $T$, and $S$ are defined as in the context-free case, but the set of production rules $P$ now includes rules of the form $A \\to \\alpha$, where $A \\in N$ is a nonterminal symbol, and $\\alpha$ is a graph with vertices and edges labeled from $N \\cup T$, subject to the condition that the embedding of the left-hand side graph into the right-hand side graph preserves the labels of the vertices and edges that are common to both graphs.\n",
    "\n",
    "A derivation in a CSGG is a sequence of graph transformations that starts from the graph consisting of a single nonterminal vertex labeled with the start symbol $S$ and successively applies production rules to replace nonterminal vertices with graphs according to the rules in $P$. However, unlike in CFGGs, the application of a production rule in a CSGG must also consider the context of the nonterminal vertex being replaced, ensuring that the labels of the vertices and edges in the surrounding graph are preserved.\n",
    "\n",
    "The language generated by a CSGG is the set of all graphs that can be derived from the start symbol while adhering to the context-sensitive production rules.\n",
    "\n",
    "**Example 2**: Consider a CSGG with $N = \\{A\\}$, $T = \\{a, b, c\\}$, and production rules $P = \\{A \\to G_1, A \\to G_2\\}$, where $G_1$ is a graph with a single vertex labeled $a$ and $G_2$ is a graph with two vertices labeled $a$ and $b$ connected by an edge labeled $a$. The production rule $A \\to G_2$ can only be applied if the nonterminal vertex $A$ is connected to a vertex labeled $c$. This grammar generates graphs with any number of vertices labeled $a$ and $b$, connected by edges labeled $a$, but only if the vertex labeled $b$ is connected to a vertex labeled $c$.\n",
    "\n",
    "**Connections to Modeling Language**\n",
    "\n",
    "Graph grammars can be used to model natural language by representing the syntactic structure of sentences as graphs and using graph transformation rules to capture the hierarchical relationships and context-dependent constraints in language. The vertices of the graph can represent words or phrases, while the edges capture syntactic dependencies between them. Nonterminal symbols in the grammar correspond to syntactic categories (e.g., noun phrases, verb phrases), and terminal symbols represent words or morphemes.\n",
    "\n",
    "One of the most well-known approaches to modeling language with graph grammars is the Dependency Grammar, where words are represented as vertices, and directed edges capture the syntactic dependencies between them. The graph transformations in this framework correspond to the generation or parsing of sentences, providing a mechanism for understanding and generating natural language structures.\n",
    "\n",
    "By training a transformer model, such as GPT-2, on tokenized graphs derived from natural language sentences, it is possible to learn the underlying graph grammar implicitly. The model can then be used to generate and analyze sentences by constructing and transforming graphs according to the learned grammar rules. This approach provides a powerful framework for representing the complex and context-sensitive nature of natural language syntax and semantics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Surface Codes\n",
    "\n",
    "Quantum surface codes are a class of quantum error-correcting codes used to protect quantum information against errors in quantum computation. The codes are defined on two-dimensional lattices or graphs embedded in Riemann surfaces, with qubits associated with either the edges, vertices, or faces of the graph. The geometric dual graph of the original graph is also embedded in the same Riemann surface.\n",
    "\n",
    "In the context of quantum surface codes, a graph $\\Gamma_1$ embedded in a Riemann surface $\\Sigma$ can represent the lattice of the surface code, and its geometric dual graph $\\Gamma_2$ can represent the dual lattice, which is also embedded in $\\Sigma$. The combined graph $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$ encodes the structure of both the original and dual lattices.\n",
    "\n",
    "By using graph-based transformer models with attention mechanisms and positional encodings, as described earlier, we can effectively process and analyze quantum surface codes. The Laplacian positional encoding captures the structural information of the graph, including the connectivity and the relative positions of nodes, which are essential for understanding the topology of the Riemann surface and the error-correcting properties of the quantum surface code.\n",
    "\n",
    "When applying the transformer model to quantum surface codes, the feature map $f$ can represent physical properties associated with the qubits, such as error probabilities, local noise correlations, or other relevant information. By incorporating this information along with the Laplacian positional encoding, the transformer can learn to identify and correct errors in the quantum surface code.\n",
    "\n",
    "In summary, the graph-based transformer model with attention mechanisms and Laplacian positional encodings can be applied to quantum surface codes represented by graphs cellularly embedded in Riemann surfaces and their geometric dual graphs. This approach enables the effective processing of quantum surface codes, taking into account both the topological structure of the Riemann surface and the physical properties of the qubits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Gareth A. Jones , Jürgen Wolfart; [Dessins d'Enfants on Riemann Surfaces](https://link.springer.com/book/10.1007/978-3-319-24711-3), Springer Monographs in Mathematics, 978-3-319-24709-0; Published 05 April 2016\n",
    "\n",
    "- Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong; [Pure Transformers are Powerful Graph Learners](https://arxiv.org/abs/2207.02505)\n",
    "\n",
    "- Sergei K. Lando, Alexander K. Zvonkin; [Graphs on Surfaces and Their Applications](https://www-users.cse.umn.edu/~reiner/Classes/Math8680Fall2014Papers/LandoZvonkin.pdf), Encyclopaedia of Mathematical Sciences Volume 141\n",
    "\n",
    "- David W. Romero, Jean-Baptiste Cordonnier; [Group Equivariant Stand-Alone Self-Attention For Vision](https://openreview.net/forum?id=JkfYjnOEo6M)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
