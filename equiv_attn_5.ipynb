{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Notes on Group Equivariant Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f \\in L_{\\mathbb{R}^d}(S)$, where $S$ is a finite set of indices, we define three functions:\n",
    "\n",
    "1. Key function $\\varphi_{key}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_k}}(S)$\n",
    "2. Query function $\\varphi_{query}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_k}}(S)$\n",
    "3. Value function $\\varphi_{value}: L_{\\mathbb{R}^d}(S) \\to L_{\\mathbb{R}^{d_v}}(S)$\n",
    "\n",
    "These functions are used to compute the self-attention for each element in $S$. The self-attention mechanism also incorporates a relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ that encodes the relative positions between elements.\n",
    "\n",
    "The self-attention function $\\alpha[f]: S \\times S \\to \\mathbb{R}$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\alpha[f](i, j) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f(i)), \\varphi_{key}(f(j)) + \\rho(i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f(i)), \\varphi_{key}(f(k)) + \\rho(i, k) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "Now, we want to extend the domain of these functions from $S$ to $\\mathcal{X}$ using the quotient space $\\mathcal{X} = G/\\mathscr{H}$, where $\\mathcal{X}$ is a homogeneous space. We have the coordinate function $x: S \\to \\mathcal{X}$ that maps elements of $S$ to $\\mathcal{X}$, and we can define $f_{\\mathcal{X}}: \\mathcal{X} \\to \\mathbb{R}^d$ such that $f_{\\mathcal{X}}(x(i)) = f(i)$. To extend the domain of the key, query, and value functions, we have:\n",
    "\n",
    "1. Key function $\\varphi_{key}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, where $\\varphi_{key}(f_{\\mathcal{X}}(x(i))) = \\varphi_{key}(f(i))$\n",
    "2. Query function $\\varphi_{query}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, where $\\varphi_{query}(f_{\\mathcal{X}}(x(i))) = \\varphi_{query}(f(i))$\n",
    "3. Value function $\\varphi_{value}: L_{\\mathbb{R}^d}(\\mathcal{X}) \\to L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$, where $\\varphi_{value}(f_{\\mathcal{X}}(x(i))) = \\varphi_{value}(f(i))$\n",
    "\n",
    "\n",
    "In the context of self-attention, the query, key, and value functions play essential roles in transforming the input features. Each of these functions has a specific domain and codomain, as described below:\n",
    "\n",
    "1. Query function ($\\varphi_{qry}$):\n",
    "\n",
    "- Domain: The input space of the query function is the feature space associated with the elements of the input set, denoted as $L_{\\mathbb{R}^d}(S)$ for the original self-attention, and $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$. In the case of group self-attention, the domain is $L_{\\mathbb{R}^d}(G)$.\n",
    "\n",
    "- Codomain: The output space of the query function is a transformed feature space, typically of dimension $d_k$. It is denoted as $L_{\\mathbb{R}^{d_k}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_k}}(G)$ for group self-attention.\n",
    "\n",
    "2. Key function ($\\varphi_{key}$):\n",
    "\n",
    "- Domain: Similar to the query function, the input space of the key function is the feature space associated with the elements of the input set, denoted as $L_{\\mathbb{R}^d}(S)$ for the original self-attention, $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^d}(G)$ for group self-attention.\n",
    "\n",
    "- Codomain: The output space of the key function is also a transformed feature space, typically of dimension $d_k$. It is denoted as $L_{\\mathbb{R}^{d_k}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_k}}(G)$ for group self-attention.\n",
    "\n",
    "3. Value function ($\\varphi_{val}$):\n",
    "\n",
    "- Domain: The input space of the value function is the same as that of the query and key functions: $L_{\\mathbb{R}^d}(S)$ for the original self-attention, $L_{\\mathbb{R}^d}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^d}(G)$ for group self-attention.\n",
    "\n",
    "- Codomain: The output space of the value function is a transformed feature space, typically of dimension $d_v$. It is denoted as $L_{\\mathbb{R}^{d_v}}(S)$ for the original self-attention, $L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$ when extended to the domain $\\mathcal{X}$, and $L_{\\mathbb{R}^{d_v}}(G)$ for group self-attention.\n",
    "\n",
    "These functions act on the input features and transform them into a suitable representation for computing self-attention, which allows the model to capture and utilize dependencies between different input elements.\n",
    "\n",
    "The notation $L_{\\mathbb{R}^d}(S)$, $L_{\\mathbb{R}^d}(\\mathcal{X})$, and $L_{\\mathbb{R}^d}(G)$ represent function spaces of functions mapping from the respective domain to the feature space $\\mathbb{R}^d$. Here, $d$ is the dimension of the feature space.\n",
    "\n",
    "1. $L_{\\mathbb{R}^d}(S)$: This represents the space of functions that map from the input set $S$ to the $d$-dimensional feature space $\\mathbb{R}^d$. In the context of self-attention, $S$ typically represents a sequence or a set of elements (e.g., words in a sentence, pixels in an image, or nodes in a graph), and the function maps each element in the set to a $d$-dimensional feature vector.\n",
    "\n",
    "2. $L_{\\mathbb{R}^d}(\\mathcal{X})$: This represents the space of functions that map from the homogeneous space $\\mathcal{X}$ to the $d$-dimensional feature space $\\mathbb{R}^d$. The homogeneous space $\\mathcal{X}$ is formed by the quotient space $G/\\mathscr{H}$, where $G$ is a group (e.g., $\\mathbb{R}^2 \\rtimes \\mathscr{H}$) and $\\mathscr{H}$ is a subgroup (e.g., $SO(2)$ or $SO(3)$). The functions in this space map each element in the homogeneous space $\\mathcal{X}$ to a $d$-dimensional feature vector.\n",
    "\n",
    "3. $L_{\\mathbb{R}^d}(G)$: This represents the space of functions that map from the group $G$ to the $d$-dimensional feature space $\\mathbb{R}^d$. In the context of group self-attention, $G$ is a group acting on the input set (e.g., translation or rotation group), and the functions in this space map each element in the group $G$ to a $d$-dimensional feature vector.\n",
    "\n",
    "For the codomains, the notations $L_{\\mathbb{R}^{d_k}}(S)$, $L_{\\mathbb{R}^{d_k}}(\\mathcal{X})$, $L_{\\mathbb{R}^{d_k}}(G)$, $L_{\\mathbb{R}^{d_v}}(S)$, $L_{\\mathbb{R}^{d_v}}(\\mathcal{X})$, and $L_{\\mathbb{R}^{d_v}}(G)$ are similar but refer to the output spaces of the respective query, key, and value functions. The output spaces are also feature spaces but with different dimensions, such as $d_k$ for the query and key functions and $d_v$ for the value function.\n",
    "\n",
    "\n",
    "Next, we need to extend the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}$. To extend the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ to the domain $\\mathcal{X} \\times \\mathcal{X}$, we define a new function $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ such that $\\rho^P(x(j) - x(i)) = \\rho(i, j)$. \n",
    "\n",
    "Now, we can define the extended self-attention function $\\alpha[f_{\\mathcal{X}}]: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ as follows:\n",
    "\n",
    "$$\n",
    "\\alpha[f_{\\mathcal{X}}](x(i), x(j)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(j))) + \\rho^P(x(j)-x(i)) \\rangle\\right)}{\\sum_{x(k) \\in \\mathcal{X}}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(k))) + \\rho^P(x(k)-x(i)) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "By extending the domain of the key, query, and value functions and the relative positional encoding to $\\mathcal{X}$, we have successfully extended the self-attention mechanism from a finite set $S$ to a homogeneous space $\\mathcal{X}$. This lays the groundwork for further generalizing the self-attention mechanism to incorporate the structure of the group $G$ and develop group self-attention using liftings. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifting Self-Attention and Group Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us proceed by explaining how to lift the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ by first extending its domain to $\\mathcal{X} \\times \\mathcal{X}$ and then defining $\\mathcal{L}[\\rho](i, j) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))$. This can be thought of as first extending the domain of $\\rho$ to $\\mathcal{X} \\times \\mathcal{X}$, then lifting $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ to $\\mathcal{L}[\\rho]: G \\to \\mathbb{R}^d$. To lift the relative positional encoding $\\rho: S \\times S \\to \\mathbb{R}^d$ to incorporate the group structure $G$, we follow these steps:\n",
    "\n",
    "1. Extend the domain of $\\rho$ to $\\mathcal{X} \\times \\mathcal{X}$ by defining a new function $\\rho^P: \\mathcal{X} \\to \\mathbb{R}^d$ such that $\\rho^P(x(j) - x(i)) = \\rho(i, j)$. \n",
    "\n",
    "2. Define a lifting function $\\mathcal{L}_h[\\rho]: S \\times S \\to \\mathbb{R}^d$ that incorporates the action of the group element $h \\in G$ on the relative positional encoding. The lifting function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_h[\\rho](i, j) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))\n",
    "$$\n",
    "\n",
    "Here, $h^{-1}$ denotes the inverse of the group element $h$. The lifting function essentially \"lifts\" the relative positional encoding to the group by applying the group action on the elements of $\\mathcal{X}$ before computing the relative positional encoding.\n",
    "\n",
    "\n",
    "To represent the lifted relative positional encoding as the concatenation of encodings indexed by $h \\in \\mathscr{H}$, we first define the lifted relative positional encoding for each element $h \\in \\mathscr{H}$ as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{h}[\\rho](x(i), x(j)) = \\rho^P(h^{-1}x(j) - h^{-1}x(i))\n",
    "$$\n",
    "\n",
    "Now, we can concatenate the lifted relative positional encodings for all elements in the subgroup $\\mathscr{H}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}[\\rho](x(i), x(j)) = \\big\\{ \\mathcal{L}_{h}[\\rho](x(i), x(j)) \\big\\}_{h \\in \\mathscr{H}}\n",
    "$$\n",
    "\n",
    "Now, we can define a lifted self-attention mechanism that incorporates the group structure. The lifted self-attention function $\\alpha_{(h)}[f_{\\mathcal{X}}]: S \\times S \\to \\mathbb{R}$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\alpha_{(h)}[f_{\\mathcal{X}}](i, j) = \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(j))) + \\mathcal{L}_h[\\rho](i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}(x(i))), \\varphi_{key}(f_{\\mathcal{X}}(x(k))) + \\mathcal{L}_h[\\rho](i, k) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "By using the lifted self-attention function $\\alpha_{(h)}[f_{\\mathcal{X}}]$, we can now capture the dependencies between input elements while taking into account the group structure $G$. In particular, we concatenate multiple lifted self-attention operations, indexed by $h \\in \\mathscr{H}$ to obtain $\\{\\alpha_{(h)}[f_{\\mathcal{X}}]\\}_{h \\in \\mathscr{H}}$. Moreover, we can write\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_{(h)}[f_{\\mathcal{X}}](i, j) &= \\frac{\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}((i, e)))), \\varphi_{key}(f_{\\mathcal{X}}((j, e))) + \\mathcal{L}_h[\\rho](i, j) \\rangle\\right)}{\\sum_{k \\in S}\\exp\\left(\\langle \\varphi_{qry}(f_{\\mathcal{X}}((i, e))), \\varphi_{key}(f_{\\mathcal{X}}((k, e))) + \\mathcal{L}_h[\\rho](i, k) \\rangle\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "where $e \\in \\mathscr{H}$ is the identity element. This generalization of the self-attention mechanism allows for more expressive modeling of complex data structures and can be particularly useful when the input data have a natural group structure, such as images with translation and rotation invariance or graphs with symmetries. Now, we would like to define the group self-attention map as \n",
    "\n",
    "$$\n",
    "\\alpha[f_G]((i, h_1), (j, h_2)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}^G(f_G(i, h_1)), \\varphi_{key}^G(f_G(j, h_2) + \\mathcal{L}[\\rho](i, j)) \\rangle\\right)}{\\sum_{(k, h_3) \\in N(i, h_1) \\subset G}\\exp\\left(\\langle \\varphi_{qry}^G(f_G(i, h_1)), \\varphi_{key}^G(f_G(k, h_3) + \\mathcal{L}[\\rho](i, k)) \\rangle\\right)},\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generalize the group self-attention to multihead group self-attention, we will incorporate multiple attention heads, each with its own key, query, and value functions. This allows the model to capture different aspects of the input data by combining the attention weights from each head.\n",
    "\n",
    "Let $[H]$ be the number of attention heads. We will denote the key, query, and value functions for each head as $\\varphi_{key}^{head}, \\varphi_{qry}^{head},$ and $\\varphi_{val}^{head}$, respectively. Now, we can define the multihead group self-attention function $\\alpha^{head}[f_G]: G \\times G \\to \\mathbb{R}$ for each head as:\n",
    "\n",
    "$$\n",
    "\\alpha^{head}[f_G]((i, h_1), (j, h_2)) = \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(i, h_1)), \\varphi_{key}^{head}(f_G(j, h_2)) + \\mathscr{L}_h [\\rho]((i, h_1), (j, h_2)) \\rangle\\right)}{\\sum_{(k, h_3) \\in N(i, h_1) \\subset G}\\exp\\left(\\langle \\varphi_{qry}^{head}(f_G(i, h_1)), \\varphi_{key}^{head}(f_G(k, h_3)) + \\mathscr{L}_h [\\rho]((i, h_1), (k, h_3)) \\rangle\\right)}\n",
    "$$\n",
    "\n",
    "For each head, we compute the corresponding attention weights and apply them to the value function, yielding a weighted sum of the value vectors. We then concatenate the results from all heads and apply an output function $\\varphi_{out}$:\n",
    "\n",
    "$$\n",
    "m_G^r[f, \\rho](i, h) = \\varphi_{out}\\left( \\bigcup_{head \\in [H]} \\sum_{h_1 \\in \\mathscr{H}} \\sum_{(j, h_2) \\in N(i,h_1)} \\alpha^{head}[f_G]((i, h_1), (j, h_2)) \\varphi_{val}^{head}(f_G(j, h_2)) \\right)\n",
    "$$\n",
    "\n",
    "This multihead group self-attention mechanism captures multiple relationships in the input data, allowing the model to better understand and represent the underlying structure. By incorporating the structure of the group $G$ and its action on the homogeneous space $\\mathcal{X}$, multihead group self-attention provides a powerful and flexible way to model data with geometric and topological structures.\n",
    "\n",
    "To reiterate, let $G = \\mathcal{X} \\rtimes \\mathscr{H}$ be an affine group acting on itself, and $f(g) = f(i, \\tilde{h}) \\in L_{\\mathbb{R}^d}(G)$, $i \\in S$, $\\tilde{h} \\in \\mathscr{H}$, be a function defined on a set immersed with the structure of the group $G$. That is, enriched with a positional encoding $\\rho((i, \\tilde{h}), (j, \\hat{h})) := \\rho_P((x(j) - x(i), \\tilde{h}^{-1} \\hat{h})), i, j \\in S, \\tilde{h}, \\hat{h} \\in \\mathscr{H}$. The group self-attention $m^r_G[f, \\rho] : L_{\\mathbb{R}^d}(G) \\to L_{\\mathbb{R}^d}(G)$ is a map from functions on $G$ to functions on $G$ obtained by modifying the group positional encoding by the action of group elements $h \\in \\mathscr{H}$: $\\{L_h [\\rho]((i, \\tilde{h}), (j, \\hat{h}))\\}_{h \\in \\mathscr{H}}$, $L_h [\\rho]((i, \\tilde{h}), (j, \\hat{h})) = \\rho_P(h^{-1}(x(j) - x(i)), h^{-1}(\\tilde{h}^{-1} \\hat{h}))$. It corresponds to the concatenation of multiple self-attention operations (Eq. 11) indexed by $h$ with varying positional encodings $L_h [\\rho]$ and followed by a summation over the output domain along $\\tilde{h}$:\n",
    "\n",
    "$$\n",
    "m^r_G[f, \\rho](i, h) = \\sum_{\\tilde{h} \\in \\mathscr{H}} m^r[f, L_h[\\rho]](i, \\tilde{h}) \\hspace{1cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\varphi_{\\text{out}} \\Bigg( \\bigcup_{h \\in [H]} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j, \\hat{h}) \\in N(i, \\tilde{h})} \\sigma_{j, \\hat{h}} \\Big\\langle \\varphi^{(h)}_{\\text{qry}}(f(i, \\tilde{h})), \\varphi^{(h)}_{\\text{key}}(f(j, \\hat{h})) + L_h[\\rho]((i, \\tilde{h}), (j, \\hat{h})) \\Big\\rangle \\varphi^{(h)}_{\\text{val}}(f(j, \\hat{h})) \\Bigg).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the given equations more readable and maintain the integrity of the content, I have added line breaks to the long lines. he following is the calculations necessary for proving that group self-atention is equivariant, \n",
    "\n",
    "\\begin{align*}\n",
    "m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j,\\hat{h}) \\in N(i, \\tilde{h})} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (L_y L_{\\bar{h}}[f](i, \\tilde{h})), \\varphi^{head}_{\\text{key}} (L_y L_{\\bar{h}}[f](j, \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(j, \\hat{h}))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(i, \\tilde{h})} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (L_y L_{\\bar{h}}[f](i, \\tilde{h})), \\varphi^{head}_{\\text{key}} (L_y L_{\\bar{h}}[f](k, \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(k, \\hat{h}))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (L_y L_{\\bar{h}}[f](j, \\hat{h})) \\\\\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(j,\\hat{h}) \\in N(i, \\tilde{h})} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1} \\tilde{h})), \\varphi^{head}_{\\text{key}} (f(x^{-1}(\\bar{h}^{-1}(x(j) - y)), \\bar{h}^{-1} \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(j, \\hat{h}))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(i, \\tilde{h})} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1} \\tilde{h})), \\varphi^{head}_{\\text{key}} (f(x^{-1}(\\bar{h}^{-1}(x(k) - y)), \\bar{h}^{-1} \\hat{h})\n",
    "+ L_h [\\rho]((i, \\tilde{h}),(k, \\hat{h}))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (f(x^{-1}(\\bar{h}^{-1}(x(j) - y)), \\bar{h}^{-1} \\hat{h})) \\\\\n",
    "&= \\phi_{\\text{out}} \\sum_{\\tilde{h} \\in \\mathscr{H}} \\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(\\bar{i}, \\tilde{h}')), \\varphi^{head}_{\\text{key}} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_h [\\rho]((x^{-1}(\\bar{h}x(\\bar{i}) + y), \\bar{h} \\tilde{h}'),\n",
    "(x^{-1}(\\bar{h}x(\\bar{j}) + y), \\bar{h} \\hat{h}'))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi^{head}_{\\text{qry}} (f(\\bar{i}, \\tilde{h}')), \\varphi^{head}_{\\text{key}} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_h [\\rho]((x^{-1}(\\bar{h}x(\\bar{i}) + y), \\bar{h} \\tilde{h}'),\n",
    "(x^{-1}(\\bar{h}x(\\bar{j}) + y), \\bar{h} \\hat{h}'))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi^{head}_{\\text{val}} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)\n",
    "&= \\phi_{out} \\sum_{h \\in \\mathscr{H}} \\sum_{\\bar{h}\\tilde{h}' \\in \\mathscr{H}}\\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ \\rho_P(h^{-1}\\bar{h}(x(\\bar{j}) - x(\\bar{i}), \\tilde{h}'^{-1} \\hat{h}'))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ \\rho_P(h^{-1}\\bar{h}(x(\\bar{j}) - x(\\bar{i}), \\tilde{h}'^{-1} \\hat{h}'))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi_{val}^{head} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "&= \\phi_{out} \\sum_{h \\in \\mathscr{H}} \\sum_{\\bar{h}\\tilde{h}' \\in \\mathscr{H}} \\sum_{(x^{-1}(\\bar{h}x(\\bar{j})+y),\\bar{h}\\hat{h}') \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\\\\n",
    "&\\quad \\frac{\\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_{\\bar{h}^{-1}h}[\\rho]((\\bar{i}, \\tilde{h}'),(\\bar{j}, \\hat{h}')))\\rangle\\right)}{\\sum_{(k, \\hat{h}) \\in N(x^{-1}(\\bar{h}x(\\bar{i})+y),\\bar{h}\\tilde{h}')} \\exp\\left(\\langle \\varphi_{qry}^{head} (f(\\bar{i}, \\tilde{h}')), \\varphi_{key}^{head} (f(\\bar{j}, \\hat{h}')\n",
    "+ L_{\\bar{h}^{-1}h}[\\rho]((\\bar{i}, \\tilde{h}'),(\\bar{j}, \\hat{h}')))\\rangle\\right)} \\\\\n",
    "&\\quad \\times \\varphi_{val}^{head} (f(\\bar{j}, \\hat{h}')) \\\\\n",
    "&= m_G^r[f, \\rho](\\bar{i}, \\bar{h}^{-1}h)\\\\\n",
    "&= m_G^r[f, \\rho](x^{-1}(\\bar{h}^{-1}(x(i) - y)), \\bar{h}^{-1}h) \\\\\n",
    "&= L_y L_{\\bar{h}}[m_G^r[f, \\rho]](i,h).\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equations describe the process of applying the multihead group self-attention mechanism in a geometric context. The main goal is to show that the multihead group self-attention operation commutes with the left action of the group $G$ on the homogeneous space $\\mathcal{X}$.\n",
    "\n",
    "The first set of equations starts by expressing the multihead group self-attention function $m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)$ for an input function $f$. It uses the output function $\\phi_{\\text{out}}$, the key, query, and value functions $\\varphi^{head}_{\\text{key}}$, $\\varphi^{head}_{\\text{qry}}$, and $\\varphi^{head}_{\\text{val}}$, respectively, and the group action $L_h [\\rho]$.\n",
    "\n",
    "The second and third lines of the first set of equations substitute the lifted functions $L_y L_{\\bar{h}}[f]$ for the original function $f$. This shows the transformation of indices with the group action.\n",
    "\n",
    "The fourth and fifth lines of the first set of equations rewrite the terms using the inverse group action $\\bar{h}^{-1}$ and the group action on the homogeneous space $\\mathcal{X}$.\n",
    "\n",
    "The second set of equations starts by repeating the expression for the multihead group self-attention function $m_G^r[L_y L_{\\bar{h}}[f], \\rho](i,h)$. \n",
    "\n",
    "The following lines of the second set of equations show that the multihead group self-attention operation $m_G^r[f, \\rho]$ commutes with the left action of the group $G$ on the homogeneous space $\\mathcal{X}$. This means that applying the group action before or after the multihead group self-attention operation yields the same result.\n",
    "\n",
    "In summary, these equations present the derivation that shows the commutativity property of the multihead group self-attention mechanism in a geometric context, which is important for understanding how this attention mechanism can be applied to model data with geometric and topological structures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Graphs Embedded in Surfaces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the formalism of dessin d'enfants because it provides a unified mathematical framework for tokenizing complicated graphs embedded in Riemann surfaces like multi-holed tori. For an introduction, please see [Graphs on Surfaces and Their Applications](https://www-users.cse.umn.edu/~reiner/Classes/Math8680Fall2014Papers/LandoZvonkin.pdf). Let $[\\sigma, \\alpha]$ be a rotation system with $\\sigma \\in S_{2n}$ a permutation giving the vertices of the graph, attaches to half-edges. Let $\\alpha \\in S_{2n}$ be a permutation that is an involution, that is $\\alpha^2 = \\alpha$, which tells use how to glue the half-edges around the \"stars\" associated to each cycle of $\\sigma$. The involution $\\alpha$ can be thought of as making the graph a bipartite graph, by labeling each full edge by a white \"edge node\" that represents one of the two-cycles of $\\alpha$. We can then color the vertex nodes of $\\sigma$ black, and the edge nodes of $\\alpha$ white, making the graph bipartite. Now, the dual graph can be computed as follows. We replace $\\sigma$ with $\\phi = (\\sigma \\alpha)^{-1}$ and forms the dual dessin $[\\phi, \\alpha]$. This corresponds to adding in face nodes according to $\\phi$, each with half-edges that are connected according to $\\alpha$. There is a node on every face, and the edges connect neighboring faces that share an edge on the boundary. This again makes $[\\phi, \\alpha]$ a bipartite, cellularly embedded graph in the Riemann surface $\\Sigma$. It is the geometric dual graph of $[\\sigma, \\alpha]$. \n",
    "\n",
    "Now, we would like to tokenize any such graph, treating vertex, edge, and face nodes as tokens, and given a vector embedding $X$, which can ve thought of as a functions $\\Phi:\\Gamma \\to \\mathbb{R}^d$, where $\\Gamma$ is the graph obtained by overlaying $[\\sigma, \\alpha]$ and $[\\phi, \\alpha]$ in the surface $\\Sigma$. We can restrict $\\Phi \\in L_{\\mathbb{R}^d}(\\Gamma)$ to the vertices $\\Phi_V: V \\to \\mathbb{R}^d$ giving vertex feature vectors $\\Phi(v) \\in \\mathbb{R}^d$ for $v \\in V$ a vertex. Similarly, we can restrict $\\Phi_E: E \\to \\mathbb{R}^d$ giving edge feature vectors $\\Phi(e) \\in \\mathbb{R}^d$ for each edge $e \\in E$. Finally, we can also restrict $\\Phi_F: F \\to \\mathbb{R}^d$ giving a function $\\Phi_F \\in L_{\\mathbb{R}^d}(F)$, where $\\Phi(f) \\in \\mathbb{R}^d$ for each face $f \\in F$. Now, restriction gives us three functions \n",
    "\n",
    "- $\\Phi_V \\in L_{\\mathbb{R}^d}(V) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "- $\\Phi_E \\in L_{\\mathbb{R}^d}(E) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "- $\\Phi_F \\in L_{\\mathbb{R}^d}(F) \\subset L_{\\mathbb{R}^d}(\\Gamma)$\n",
    "\n",
    "and we have $\\Phi \\in L_{\\mathbb{R}^d}(\\Gamma)$ where $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$, where $\\Gamma_1$ is the orginial graph embedded in the Riemann surface according to $[\\sigma, \\alpha]$ and $\\Gamma_2 = \\Gamma_1^{\\vee}$ is the geometric dual graph of $\\Gamma_1$ and ided n the surface according to $[\\phi, \\alpha]$. With the feature function $\\Phi$ defined on $\\Gamma$ in hand, we can define three projection maps, \n",
    "\n",
    "\n",
    "- $\\varphi_{qry}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_k}}(\\Gamma)$\n",
    "- $\\varphi_{key}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_k}}(\\Gamma)$\n",
    "- $\\varphi_{val}: L_{\\mathbb{R}^d}(\\Gamma) \\to L_{\\mathbb{R}^{d_v}}(\\Gamma)$\n",
    "\n",
    "\n",
    "and we can define attention as before. With the feature function $\\Phi$ defined on $\\Gamma$ and the projection maps $\\varphi_{qry}$, $\\varphi_{key}$, and $\\varphi_{val}$ in hand, we can now define the attention mechanism for the graph embedded in the Riemann surface $\\Sigma$. We can extend the previously defined multihead self-attention mechanism to work on the cellularly embedded graph $\\Gamma$ by considering the three types of nodes: vertex (V), edge (E), and face (F) nodes.\n",
    "\n",
    "Let's define the self-attention function for each node type as:\n",
    "\n",
    "1. $\\alpha[f_{\\mathcal{X}}](x(i), x(j))$\n",
    "\n",
    "For each node type, we compute the self-attention as follows:\n",
    "\n",
    "$$\n",
    "\\text{SA}^{head}[f_{\\mathcal{X}}](x(i), x(j)) = \\sum_{j \\in N(i) \\subseteq S}\\alpha^{head}[f_{\\mathcal{X}}](x(i), x(j)) \\cdot \\varphi_{val}^{head}(f_{\\mathcal{X}}(x(j))) \n",
    "$$\n",
    "\n",
    "Using the $\\bigcup_{h \\in [H]}$ notation and applying the function $\\phi_{out}$, the full multi-head self-attention formula can be written as:\n",
    "\n",
    "$$\n",
    "\\text{MHA}[f_{\\mathcal{X}}](x(i)) = \\phi_{out}\\left(\\bigcup_{h \\in [H]}\\left(\\sum_{j \\in N(i) \\subseteq S} \\alpha^h[f_{\\mathcal{X}}](x(i), x(j)) \\cdot \\varphi_{val}^h(f_{\\mathcal{X}}(x(j)))\\right)\\right)\n",
    "$$\n",
    "\n",
    "In this formula, we compute the self-attention values $\\alpha^h[f_{\\mathcal{X}}](x(i), x(j))$ and the corresponding values from the value projection $\\varphi_{val}^h(f_{\\mathcal{X}}(x(j)))$ for each attention head $h$. We sum these values over all neighbors $j \\in N(i) \\subseteq S$, then concatenate the results from all attention heads in the set $[H]$. Finally, we apply the output function $\\phi_{out}$ to the concatenated result to obtain the final multi-head self-attention output for the node $x(i)$.\n",
    "\n",
    "Applying attention mechanisms to graphs embedded in Riemann surfaces, such as the torus code, requires positional encodings that can capture the topology and geometry of the graph and its dual. There are several types of positional encodings that might be appropriate for this task. Let's consider the options you've mentioned:\n",
    "\n",
    "1. Graph Laplacian positional encodings: Graph Laplacian positional encodings can capture the local connectivity of a graph, including its dual, and can be used to encode the position of nodes relative to each other. They are computed using the graph Laplacian matrix, which considers the connectivity of the graph. These encodings can capture local information and might be a suitable choice for the attention mechanism applied to the tokenization of a graph and its dual. However, they might not fully capture the global structure of the graph and its embedding in a Riemann surface.\n",
    "\n",
    "2. Relative positional encodings: Relative positional encodings are designed to capture the relative position of elements in a sequence or structure, making them suitable for encoding the position of nodes in the torus code. Since the torus code is essentially an image pixel grid on the torus, and relative positional encodings have been shown to work well for images, they could be an appropriate choice. However, it is crucial to adapt the relative positional encodings to the graph setting and take into account the specific structure of the torus code, including vertex, edge, and face nodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Tokenizing Graphs \n",
    "\n",
    "Following the ideas of [Pure Transformers are Powerful Graph Learners](https://arxiv.org/abs/2207.02505), we tokenize vertex nodes, edges nodes, and face nodes on equal footing, similar to the following image, except there is a \"third type\" idenifier in addition to \"$v$\" and \"$e\"$, denoyted by \"$f$\". We make one modification, by adding an additional channel for each type of feature we want to add. For example, in the appliation to quantum error correction, we may wish to add a channel for each type of error that can occur. This might include $X$ and $Z$ errors, measurement errors, or two qubit gate errors, which can now be represented as tokenized edges. The following was taken from [Pure Transformers are Powerful Graph Learners](https://arxiv.org/abs/2207.02505):\n",
    "\n",
    "<img src=\"/Users/amelieschreiber/vscode_projects/equivariant_attention/tokenizing_graphs.png\"  width=\"800\" height=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Gamma_1$ be a graph embedded in a Riemann surface $\\Sigma$ according to the rotation system $[\\sigma, \\alpha]$, where $\\sigma \\in S_{2n}$ is a permutation representing the cyclic order of edges around each vertex, and $\\alpha \\in S_{2n}$ is an involution representing the gluing of half-edges around vertices. Let $\\Gamma_2 = \\Gamma_1^{\\vee}$ be the geometric dual graph of $\\Gamma_1$, also embedded in $\\Sigma$ according to the rotation system $[\\phi, \\alpha]$.\n",
    "\n",
    "We define a new graph $\\Gamma$ as the union of the two graphs, $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$, where each vertex, edge, and face of the embedded graphs are represented by nodes in $\\Gamma$. We will label each node in $\\Gamma$ with five labels: \"tail1\", \"head1\", \"tail2\", \"head2\", and \"type\".\n",
    "\n",
    "To rigorously describe the labeling process, let $V$, $E$, and $F$ represent the sets of vertices, edges, and faces, respectively, of the graphs $\\Gamma_1$ and $\\Gamma_2$. For each node $x \\in \\Gamma$, we define the labels as follows:\n",
    "\n",
    "1. If $x \\in V$, then the node represents a vertex of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the index of the vertex, denoted as $i_v(x)$. Similarly, we set the labels \"tail2\" and \"head2\" equal to $i_v(x)$ as well. The \"type\" label is set to \"$v$\".\n",
    "\n",
    "2. If $x \\in F$, then the node represents a face of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the index of the face, denoted as $i_f(x)$. Likewise, we set the labels \"tail2\" and \"head2\" equal to $i_f(x)$. The \"type\" label is set to \"$f$\".\n",
    "\n",
    "3. If $x \\in E$, then the node represents an edge of the embedded graphs. We set the labels \"tail1\" and \"head1\" equal to the indices of the vertex nodes connected by the edge, denoted as $i_v^t(x)$ and $i_v^h(x)$, respectively. We set the labels \"tail2\" and \"head2\" equal to the indices of the face nodes connected by the edge, denoted as $i_f^t(x)$ and $i_f^h(x)$, respectively. The \"type\" label is set to \"$e$\".\n",
    "\n",
    "In summary, we have defined a graph $\\Gamma$ as the union of two embedded graphs $\\Gamma_1$ and $\\Gamma_2$, with each node in $\\Gamma$ representing either a vertex, edge, or face of the original graphs. We assign five labels to each node in $\\Gamma$ according to its role in the structure of the embedded graphs, providing a rigorous and clear description suitable for inclusion in a research article on deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Applications\n",
    "\n",
    "### Molecular Graphs\n",
    "\n",
    "To apply the Riemann surface-based graph attention mechanism to molecular property prediction, we first need to represent the molecular structure as a graph embedded in a Riemann surface. Here's a step-by-step explanation:\n",
    "\n",
    "1. Molecular graph representation: Start by representing the molecule as a graph $\\Gamma(M)$, where the atoms in the molecule are the vertices and the chemical bonds between the atoms are the edges. Each vertex is associated with a feature vector representing the atom's properties (e.g., atomic number, electronegativity), and each edge is associated with a feature vector representing the bond properties (e.g., bond type, bond order).\n",
    "\n",
    "2. Rotation system representation: In order to obtain a rotation system $[\\sigma, \\alpha]$ for the graph $\\Gamma(M)$, we need to assign a cyclic order to the edges incident to each vertex and an involution to the half-edges. \n",
    "\n",
    "   (a) For each vertex, pick an arbitrary starting edge and then order the incident edges in a clockwise or counterclockwise direction. This ordering defines the permutation $\\sigma \\in S_{2n}$, which represents the vertices and their connections to half-edges.\n",
    "\n",
    "   (b) Define the involution $\\alpha \\in S_{2n}$ by pairing each half-edge with the half-edge on the opposite side of the bond it belongs to. Since each bond connects two half-edges, $\\alpha$ will always be an involution, i.e., $\\alpha^2 = \\alpha$.\n",
    "\n",
    "3. Dual graph and dessin d'enfants: Compute the dual graph $[\\phi, \\alpha]$ by replacing $\\sigma$ with $\\phi = (\\sigma \\alpha)^{-1}$, forming a bipartite cellularly embedded graph in the Riemann surface $\\Sigma$. The dessin d'enfants representation allows us to work with a unified mathematical framework for the graph and its dual.\n",
    "\n",
    "4. Tokenization: Tokenize the graph $\\Gamma(M)$ by considering vertex, edge, and face nodes. Assign feature vectors to each type of node using the previously mentioned feature functions $\\Phi_V$, $\\Phi_E$, and $\\Phi_F$.\n",
    "\n",
    "5. Riemann surface-based graph attention mechanism: Apply the previously defined multihead self-attention mechanism on the tokenized graph, considering the three types of nodes (vertex, edge, and face nodes). Use suitable positional encodings to capture the relationships between nodes, considering the graph structure and the embedding in the Riemann surface.\n",
    "\n",
    "6. Aggregation and prediction: After applying the graph attention mechanism, aggregate the node features to obtain a single vector representing the entire molecular graph. This can be done using techniques such as pooling, summing, or averaging the node features. Finally, use a task-specific output layer to make predictions for the molecular properties of interest (e.g., solubility, toxicity).\n",
    "\n",
    "By following these steps, we can apply the Riemann surface-based graph attention mechanism to molecular property prediction. The key aspect is the ability to capture complex topological and geometrical properties of the molecular graph, which can help improve the model's performance in predicting various properties of the molecule. We also note possible applications to quantum error crrection. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Surface Codes\n",
    "\n",
    "Quantum surface codes are a class of quantum error-correcting codes used to protect quantum information against errors in quantum computation. The codes are defined on two-dimensional lattices or graphs embedded in Riemann surfaces, with qubits associated with either the edges, vertices, or faces of the graph. The geometric dual graph of the original graph is also embedded in the same Riemann surface.\n",
    "\n",
    "In the context of quantum surface codes, a graph $\\Gamma_1$ embedded in a Riemann surface $\\Sigma$ can represent the lattice of the surface code, and its geometric dual graph $\\Gamma_2$ can represent the dual lattice, which is also embedded in $\\Sigma$. The combined graph $\\Gamma = \\Gamma_1 \\cup \\Gamma_2$ encodes the structure of both the original and dual lattices.\n",
    "\n",
    "By using graph-based transformer models with attention mechanisms and positional encodings, as described earlier, we can effectively process and analyze quantum surface codes. The Laplacian positional encoding captures the structural information of the graph, including the connectivity and the relative positions of nodes, which are essential for understanding the topology of the Riemann surface and the error-correcting properties of the quantum surface code.\n",
    "\n",
    "When applying the transformer model to quantum surface codes, the feature map $f$ can represent physical properties associated with the qubits, such as error probabilities, local noise correlations, or other relevant information. By incorporating this information along with the Laplacian positional encoding, the transformer can learn to identify and correct errors in the quantum surface code.\n",
    "\n",
    "In summary, the graph-based transformer model with attention mechanisms and Laplacian positional encodings can be applied to quantum surface codes represented by graphs cellularly embedded in Riemann surfaces and their geometric dual graphs. This approach enables the effective processing of quantum surface codes, taking into account both the topological structure of the Riemann surface and the physical properties of the qubits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Gareth A. Jones , JÃ¼rgen Wolfart; [Dessins d'Enfants on Riemann Surfaces](https://link.springer.com/book/10.1007/978-3-319-24711-3), Springer Monographs in Mathematics, 978-3-319-24709-0; Published 05 April 2016\n",
    "\n",
    "- Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, Seunghoon Hong; [Pure Transformers are Powerful Graph Learners](https://arxiv.org/abs/2207.02505)\n",
    "\n",
    "- Sergei K. Lando, Alexander K. Zvonkin; [Graphs on Surfaces and Their Applications](https://www-users.cse.umn.edu/~reiner/Classes/Math8680Fall2014Papers/LandoZvonkin.pdf), Encyclopaedia of Mathematical Sciences Volume 141\n",
    "\n",
    "- David W. Romero, Jean-Baptiste Cordonnier; [Group Equivariant Stand-Alone Self-Attention For Vision](https://openreview.net/forum?id=JkfYjnOEo6M)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
