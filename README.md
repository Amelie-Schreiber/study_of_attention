# study_of_attention
A study of attention from many perspectives

In these notebooks we study:

- Permutation Equivariance of Self-Attention 
- General Group Equivariance of Self-Attention
- Attention Applied to Graphs Embedded in Surfaces and Dessins d'Enfant (with potential applications)
- Visualizing Attention Matrices and Graphs
- Basic Information Theory of Attention Probability Distributions
- Contextual Mappings and Context Vectors in GPT-2, Bert, and ViT
- Visualizing the Autoregressice Property of GPT-2
- Can we view attention as a kernel, and use quantum kernel methods to enhance performance of attention? 
