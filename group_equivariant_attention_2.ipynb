{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem with Proof of the Equivariance of Group Self-Attention\n",
    "\n",
    "In [Group Equivariant Stand-Alone Self-Attention For Vision](https://openreview.net/forum?id=JkfYjnOEo6M&noteId=dxKhFZNxn-D), there are a few calculation errors in the proof of the equivariance of group self-attention layers. Below is a and explanation of why the proof of the equivariance property does not work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $L_g[f](i, h_1) = L_yL_{h_3}[f](i, h_1) = f(x^{-1}(h_3^{-1}(x(i)-y)), h_3^{-1}h_1)$ be a $g$-transformed input signal, where $g = (y, h_3) \\in G = \\mathbb{R}^{d_{in}} \\rtimes \\mathcal{H}$. The group self-attention operation on $L_g[f]$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "m_G^r[L_yL_{h_3}[f], \\rho](i, h_1) &= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h \\in \\mathcal{H}} \\sum_{(j, h_2) \\in N(i, h_1)} \\sigma_{(j, h_2)}\\Bigg( \\langle \\varphi_{qry}^{head}L_yL_{h_3}[f](i, h_1) , \\varphi_{key}^{head}(L_yL_{h_3}[f](j, h_2) \\\\\n",
    "&\\quad + L_h[\\rho]((i, h_1), (j, h_2)) ) \\rangle \\Bigg)\\varphi_{val}^{head}(L_yL_{h_3}[f](j, h_2)) \\Bigg)\\\\\n",
    "&= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h \\in \\mathcal{H}} \\sum_{(j, h_2) \\in N(i, h_1)} \\sigma_{(j, h_2)} \\Bigg( \\langle \\varphi_{qry}^{head}f(x^{-1}(h_3^{-1} (x(i)-y)), h_3^{-1}h_1) , \\varphi_{key}^{head}(f(x^{-1}(h_3^{-1}(x(i)-y)), h_3^{-1}h_2) \\\\ \n",
    "&\\quad + L_h[\\rho]((i, h_1), (j, h_2)) ) \\rangle \\Bigg) \\varphi_{val}^{head}(f(x^{-1}(h_3^{-1}(x(i)-y)), h_3^{-1}h_2)) \\Bigg)\\\\\n",
    "&= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h_3h' \\in \\mathcal{H}} \\sum_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2') \\in N((x^{-1}(h_3x(\\overline{i})+y), h_3h_1'))} \\sigma_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2')}\\Bigg( \\langle \\varphi_{qry}^{head}f(\\overline{i}, h_1') , \\varphi_{key}^{head}(f(\\overline{j}, h_2') \\\\\n",
    "&\\quad + L_h[\\rho]((x^{-1}(h_3^{-1}x(\\overline{i})+y), h_3h_1'), (x^{-1}(h_3^{-1}x(\\overline{j})+y), h_3h_2')) ) \\rangle \\Bigg)\\varphi_{val}^{head}(f(\\overline{j}, h_2')) \\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "Here, we have used $\\overline{i} = x^{-1}(h_3^{-1}(x(i)-y)) \\implies i = x^{-1}(h_3^{-1}x(\\overline{i})+y)$, and $\\overline{j} = x^{-1}(h_3^{-1}(x(j)-y)) \\implies j = x^{-1}(h_3^{-1}x(\\overline{j})+y)$, and $h_1' = h_3^{-1}h_1$ and $h_2' = h_3^{-1}h_2$. By using the definition of $\\rho((i, h_1), (j, h_2))$ we can further reduce the equations:\n",
    "\n",
    "\\begin{align}\n",
    "&= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h_3h' \\in \\mathcal{H}} \\sum_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2') \\in N((x^{-1}(h_3x(\\overline{i})+y), h_3h_1'))} \\sigma_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2')}\\Bigg( \\langle \\varphi_{qry}^{head}f(\\overline{i}, h_1') , \\varphi_{key}^{head}(f(\\overline{j}, h_2') \\\\\n",
    "&\\quad + \\rho^P(h^{-1}(h_3x(\\overline{j})+y - (h_3x(\\overline{i})+y)), h^{-1}(h_3h_1')^{-1}(h_3h_2')) \\rangle \\Bigg)\\varphi_{val}^{head}(f(\\overline{j}, h_2')) \\Bigg)\\\\\n",
    "&= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h_3h' \\in \\mathcal{H}} \\sum_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2') \\in N((x^{-1}(h_3x(\\overline{i})+y), h_3h_1'))} \\sigma_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2')}\\Bigg( \\langle \\varphi_{qry}^{head}f(\\overline{i}, h_1') , \\varphi_{key}^{head}(f(\\overline{j}, h_2') \\\\\n",
    "&\\quad + \\rho^P(h^{-1}(h_3x(\\overline{j}) - h_3x(\\overline{i})), h^{-1}{h'}_1^{-1}h_2')) \\rangle \\Bigg)\\varphi_{val}^{head}(f(\\overline{j}, h_2')) \\Bigg) \\quad \\text{notice the lack of a factor of } h_3^{-1} \\text{ in front of } {h'}_1^{-1}h_2' \\text{ here}\\\\\n",
    "&= \\varphi_{out} \\Bigg( \\bigcup_{head \\in [H]} \\sum_{h_3h' \\in \\mathcal{H}} \\sum_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2') \\in N((x^{-1}(h_3x(\\overline{i})+y), h_3h_1'))} \\sigma_{(x^{-1}(h_3x(\\overline{j})+y), h_3h_2')}\\Bigg( \\langle \\varphi_{qry}^{head}f(\\overline{i}, h_1') , \\varphi_{key}^{head}(f(\\overline{j}, h_2') \\\\\n",
    "&\\quad + L_{h_3^{-1}h}[\\rho](\\overline{i}, h_1'), (\\overline{j}, h_2')) \\rangle \\Bigg)\\varphi_{val}^{head}(f(\\overline{j}, h_2')) \\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "So, we see that $m_G^r[L_yL_{h_3}[f], \\rho](i, h) \\neq L_yL_{h_3}[m_G^r[f, \\rho]](i, h)$. It is also said in [Group Equivariant Stand-Alone Self-Attention For Vision](https://openreview.net/forum?id=JkfYjnOEo6M&noteId=dxKhFZNxn-D) that the equivariance of group self-attention is due to the relative positional encoding being **invariant** to the group action. That is, it is claimed to follow from $L_g[\\rho](i, j) = \\rho(i, j)$. This however is false. In particular, the positional encoding $\\rho(i, j)$ is not used in the proof, it is the positional encoding $\\rho((i, h_1), (j, h_2))$ that is used. Furthermore, we see that it is not $G$-invariant unless $x(j)-x(i)$ is an $H$-invariant vector. \n",
    "\n",
    "\\begin{align}\n",
    "L_g[\\rho]((i, h_1), (j, h_2)) &= L_yL_h[\\rho]((i, h_1), (j, h_2))\\\\\n",
    "                              &= L_h[L_y[\\rho]]((i, h_1), (j, h_2))\\\\\n",
    "                              &= L_h[\\rho]((x^{-1}(x(i)-y), h_1), (x^{-1}(x(j)-y), h_2))\\\\\n",
    "                              &= \\rho((x^{-1}(h^{-1}(x(i)-y)), h^{-1}h_1), (x^{-1}(h^{-1}(x(j)-y)), h^{-1}h_2))\\\\\n",
    "                              &= \\rho^P(h^{-1}(x(j)-y) - h^{-1}(x(i)-y) , (h^{-1}h_1)^{-1}(h^{-1}h_2))\\\\\n",
    "                              &= \\rho^P(h^{-1}(x(j)-x(i)) , (h^{-1}h_1)^{-1}(h^{-1}h_2))\\\\\n",
    "                              &= \\rho^P(h^{-1}(x(j)-x(i)), h_1^{-1}hh^{-1}h_2)\\\\\n",
    "                              &= \\rho^P(h^{-1}(x(j)-x(i)), h_1^{-1}h_2) \\\\\n",
    "                              &\\neq \\rho((i, h_1), (j, h_2)) \\quad \\text{unless } h^{-1}(x(j)-x(i)) = x(j)-x(i)\n",
    "\\end{align}\n",
    "\n",
    "This means that group self-attention is not group equivariant, as is. The other proofs in the paper do in fact work, however. Thus, it is my recommendation that we use translation equivariant group self-attention (with relative positional encoding), and use data augmentation to train in the desired equivariance until a proper equivariant layer can be constructed. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
