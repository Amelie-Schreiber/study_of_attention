# study_of_attention
A study of attention from many perspectives

In these notebooks we study:

- Permutation Equivariance of Self-Attention (without positional encodings)
- Translation Equivariance with LoRAs Proof (see [LoRA_equivariance](https://github.com/Amelie-Schreiber/study_of_attention/blob/main/LoRA_equivariance.ipynb))
- Designing General Group Equivariance of Lifting Self-Attention with LoRAs (see [LoRAs_lifting_self_attention](https://github.com/Amelie-Schreiber/study_of_attention/blob/main/LoRAs_lifting_self_attention.ipynb))
- Group Self-Attention Equivariance Proof Problem (see [group_equivariant_attention_2](https://github.com/Amelie-Schreiber/study_of_attention/blob/main/group_equivariant_attention_2.ipynb))
- Attention Applied to Graphs Embedded in Surfaces and Dessins d'Enfant (with potential applications)
- Visualizing Attention Matrices and Graphs
- Basic Information Theory of Attention Probability Distributions
- Contextual Mappings and Context Vectors in GPT-2, Bert, and ViT
- Visualizing the Autoregressive Property of GPT-2
- Can we view attention as a kernel (see for example [Transformer Dissection: A Unified Understanding of Transformerâ€™s Attention via the Lens of Kernel](https://arxiv.org/pdf/1908.11775.pdf) and then can we use quantum kernel methods to enhance performance of attention? 
- Viewing transformers as "powerful graph learners", and tokenizing (edges and vertices of) graphs, and dessins d'enfant. 
- How do we relate this to graph grammars and language modelling? 
- What about graph completion, and graph property prediction? 
